# AI Scientist Challenge å¼€å‘æ–‡æ¡£

## é¡¹ç›®æ¦‚è¿°

### é¡¹ç›®èƒŒæ™¯
æˆ‘ä»¬æ­£åœ¨å‚ä¸AI Scientist Challengeæ¯”èµ›ï¼ŒåŒ…å«**Research Ideation**ï¼ˆç ”ç©¶æ„æ€ï¼‰å’Œ**Paper Review**ï¼ˆè®ºæ–‡è¯„å®¡ï¼‰ä¸¤ä¸ªèµ›é“ã€‚é¡¹ç›®éœ€è¦æ„å»ºæ™ºèƒ½AIä»£ç†ï¼Œåœ¨ä¸¥æ ¼çš„æŠ€æœ¯çº¦æŸä¸‹å®Œæˆå­¦æœ¯ç ”ç©¶å’Œè¯„å®¡ä»»åŠ¡ã€‚

### æ ¸å¿ƒç›®æ ‡
- **Research Ideation**ï¼šåŸºäºç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆåˆ›æ–°æ€§ç ”ç©¶æƒ³æ³•
- **Paper Review**ï¼šå¯¹å­¦æœ¯è®ºæ–‡è¿›è¡Œç»“æ„åŒ–è¯„å®¡
- **æŠ€æœ¯åˆè§„**ï¼šå®Œå…¨éµå¾ªæ¯”èµ›è§„åˆ™å’Œçº¦æŸæ¡ä»¶
- **é«˜æ€§èƒ½**ï¼šåœ¨ä¸¥æ ¼çš„è¶…æ—¶é™åˆ¶å†…å®Œæˆå¤æ‚ä»»åŠ¡

### è®¾è®¡ç†å¿µ
1. **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¸…æ™°çš„èŒè´£åˆ†ç¦»ï¼Œä¾¿äºæµ‹è¯•å’Œç»´æŠ¤
2. **å†³ç­–å¾ªç¯**ï¼šæ™ºèƒ½çš„æ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿå¾ªç¯
3. **æ¸è¿›å¼è¾“å‡º**ï¼šå®æ—¶å±•ç¤ºæ€è€ƒè¿‡ç¨‹ï¼Œæå‡ç”¨æˆ·ä½“éªŒ
4. **å¥å£®æ€§**ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥

---

## ç³»ç»Ÿæ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API å±‚        â”‚    â”‚   æ™ºèƒ½ä½“å±‚        â”‚    â”‚   æœåŠ¡å±‚         â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ FastAPIè·¯ç”±   â”‚â—„â”€â”€â–ºâ”‚ â€¢ IdeationAgent  â”‚â—„â”€â”€â–ºâ”‚ â€¢ å­¦æœ¯æ£€ç´¢æœåŠ¡   â”‚
â”‚ â€¢ è¯·æ±‚éªŒè¯      â”‚    â”‚ â€¢ ReviewAgent    â”‚    â”‚ â€¢ æ–‡æ¡£å¤„ç†æœåŠ¡   â”‚
â”‚ â€¢ æµå¼å“åº”      â”‚    â”‚ â€¢ å†³ç­–å¾ªç¯æ§åˆ¶    â”‚    â”‚ â€¢ åµŒå…¥è®¡ç®—æœåŠ¡   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                         â”‚
                              â–¼                         â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   å·¥å…·å±‚          â”‚    â”‚   æ•°æ®å±‚         â”‚
                    â”‚                  â”‚    â”‚                 â”‚
                    â”‚ â€¢ å­¦æœ¯APIå·¥å…·     â”‚    â”‚ â€¢ å‘é‡å­˜å‚¨       â”‚
                    â”‚ â€¢ PDFå¤„ç†å·¥å…·    â”‚    â”‚ â€¢ ç¼“å­˜ç³»ç»Ÿ       â”‚
                    â”‚ â€¢ è¯„ä¼°å·¥å…·       â”‚    â”‚ â€¢ ä¼šè¯å­˜å‚¨       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æŠ€æœ¯æ ˆçº¦æŸ
- **åŸºç¡€ç¯å¢ƒ**ï¼šPython 3.12, FastAPI, Docker
- **æ¨¡å‹æœåŠ¡**ï¼šDeepSeek Chat/Reasoner, Alibaba Text-Embedding-V4
- **å­¦æœ¯API**ï¼šOpenAlex, Semantic Scholar, arXivç­‰ï¼ˆä»…å…è®¸åˆ—è¡¨ï¼‰
- **éƒ¨ç½²è¦æ±‚**ï¼šç«¯å£3000ï¼Œä¸­å›½å¤§é™†ç½‘ç»œç¯å¢ƒ

---

## æ ¸å¿ƒç»„ä»¶è¯¦ç»†è®¾è®¡

### 1. æ™ºèƒ½ä½“åŸºç±» (AgentBase)

#### èŒè´£
- æä¾›ç»Ÿä¸€çš„æ™ºèƒ½ä½“æ¥å£
- ç®¡ç†æµå¼å“åº”å’Œé”™è¯¯å¤„ç†
- æ§åˆ¶è¶…æ—¶å’Œèµ„æºä½¿ç”¨

#### å…³é”®è®¾è®¡
```python
class AgentBase(ABC):
    # æ ¸å¿ƒå±æ€§
    - client: AsyncOpenAI           # LLMå®¢æˆ·ç«¯
    - embedding_client: AsyncOpenAI # åµŒå…¥å®¢æˆ·ç«¯  
    - memory: SessionMemory         # ä¼šè¯è®°å¿†
    - config: AgentConfig          # é…ç½®ç®¡ç†
    
    # æŠ½è±¡æ¥å£
    + async execute(query: str, **kwargs) -> AsyncGenerator[str, None]
    + async _thinking_loop() -> List[ThoughtStep]
    + async _fallback_strategy() -> AsyncGenerator[str, None]
    
    # å·¥å…·æ–¹æ³•
    + _format_sse_data(content: str) -> str
    + _handle_timeout() -> None
    + _validate_input() -> bool
```

#### å®ç°è¦ç‚¹
- ä½¿ç”¨å¼‚æ­¥ç”Ÿæˆå™¨å®ç°æµå¼è¾“å‡º
- ç»Ÿä¸€çš„SSEæ•°æ®æ ¼å¼å°è£…
- è¶…æ—¶æ§åˆ¶ä½¿ç”¨asyncio.wait_for
- å¼‚å¸¸å¤„ç†åŒ…å«è¯¦ç»†çš„é”™è¯¯åˆ†ç±»

### 2. å­¦æœ¯æ•°æ®æ£€ç´¢æœåŠ¡ (AcademicDataService)

#### æ•°æ®æµè®¾è®¡
```
ç”¨æˆ·æŸ¥è¯¢ â†’ æŸ¥è¯¢åˆ†æ â†’ å¤šAPIå¹¶è¡Œæ£€ç´¢ â†’ ç»“æœå»é‡ â†’ ç›¸å…³æ€§æ’åº â†’ è¿”å›Top-K
```

#### ç»„ä»¶è®¾è®¡
```python
class AcademicDataService:
    - cache: EmbeddingCache           # å‘é‡ç¼“å­˜
    - rate_limiter: TokenBucket       # é™æµå™¨
    - api_clients: Dict[str, Client]  # APIå®¢æˆ·ç«¯æ˜ å°„
    
    + async search_related_work(query: str, domain: str) -> List[Paper]
    + async get_paper_details(paper_id: str) -> PaperDetails
    + async find_similar_papers(embedding: List[float]) -> List[Paper]
    
    # å†…éƒ¨æ–¹æ³•
    - _build_search_queries(keywords: List[str]) -> List[str]
    - _deduplicate_papers(papers: List[Paper]) -> List[Paper]  
    - _rank_by_relevance(papers: List[Paper], query_embedding: List[float]) -> List[Paper]
```

#### APIé›†æˆç­–ç•¥
- **OpenAlex**ï¼šé¢†åŸŸè¶‹åŠ¿å’Œå¼•ç”¨ç½‘ç»œ
- **Semantic Scholar**ï¼šè®ºæ–‡ç†è§£å’Œå…³è”ç ”ç©¶
- **arXiv**ï¼šæœ€æ–°é¢„å°æœ¬æ£€ç´¢
- **Crossref**ï¼šå…ƒæ•°æ®å’Œå¼•ç”¨ä¿¡æ¯
- å®ç°è¯·æ±‚é™æµå’Œé”™è¯¯å¤„ç†

### 3. æ–‡æ¡£å¤„ç†å¼•æ“ (DocumentProcessor)

#### å¤„ç†æµæ°´çº¿
```
PDF Base64 â†’ è§£ç  â†’ PyPDF2è§£æ â†’ æ–‡æœ¬æ¸…ç† â†’ ç»“æ„è¯†åˆ« â†’ åˆ†å—å¤„ç† â†’ å…³é”®ä¿¡æ¯æå–
```

#### ç»„ä»¶è®¾è®¡
```python
class DocumentProcessor:
    - chunk_size: int = 2000         # åˆ†å—å¤§å°
    - chunk_overlap: int = 200       # é‡å åŒºåŸŸ
    - max_pages: int = 50           # æœ€å¤§é¡µæ•°é™åˆ¶
    
    + extract_text(pdf_b64: str) -> str
    + chunk_document(text: str) -> List[DocumentChunk]
    + identify_structure(chunks: List[DocumentChunk]) -> DocumentStructure
    + extract_key_elements(structure: DocumentStructure) -> PaperElements
    
    # å†…éƒ¨è§£æå™¨
    - _parse_abstract(chunks: List[DocumentChunk]) -> str
    - _parse_methodology(chunks: List[DocumentChunk]) -> str
    - _parse_experiments(chunks: List[DocumentChunk]) -> str
    - _parse_references(chunks: List[DocumentChunk]) -> List[str]
```

#### å¥å£®æ€§è®¾è®¡
- å¤„ç†åŠ å¯†PDFçš„é™çº§ç­–ç•¥
- è¡¨æ ¼å’Œå›¾ç‰‡çš„å ä½å¤„ç†
- ç¼–ç é”™è¯¯çš„è‡ªåŠ¨æ£€æµ‹å’Œä¿®å¤
- å†…å­˜ä½¿ç”¨ç›‘æ§å’Œé™åˆ¶

### 4. åµŒå…¥å’Œç›¸ä¼¼åº¦æœåŠ¡ (EmbeddingService)

#### æ¶æ„è®¾è®¡
```python
class EmbeddingService:
    - cache: TTLCache                # ç¼“å­˜å±‚
    - batch_size: int = 32          # æ‰¹å¤„ç†å¤§å°
    - max_retries: int = 3          # é‡è¯•æ¬¡æ•°
    
    + async get_embedding(text: str) -> List[float]
    + async get_embeddings_batch(texts: List[str]) -> List[List[float]]
    + calculate_similarity(vec1: List[float], vec2: List[float]) -> float
    + find_most_similar(query_embedding: List[float], candidates: List[EmbeddedItem]) -> List[SimilarityResult]
    
    # æ€§èƒ½ä¼˜åŒ–
    - _batch_processing(texts: List[str]) -> List[List[float]]
    - _cache_lookup(text: str) -> Optional[List[float]]
    - _validate_embedding(embedding: List[float]) -> bool
```

#### ä¼˜åŒ–ç­–ç•¥
- æ‰¹é‡å¤„ç†å‡å°‘APIè°ƒç”¨
- TTLç¼“å­˜é¿å…é‡å¤è®¡ç®—
- å‘é‡å½’ä¸€åŒ–ä¿è¯ç›¸ä¼¼åº¦å‡†ç¡®æ€§
- é™ç»´å¤„ç†é•¿æ–‡æœ¬çš„åµŒå…¥è®¡ç®—

---

## Research Ideation èµ›é“å®ç°

### å†³ç­–å¾ªç¯è®¾è®¡

#### é˜¶æ®µ1ï¼šæŸ¥è¯¢ç†è§£ä¸åˆ†æ
```python
class QueryAnalyzer:
    async def analyze(self, query: str) -> QueryAnalysis:
        # 1. è¯­ä¹‰åˆ†æ
        embedding = await embedding_service.get_embedding(query)
        domain = await self._classify_domain(embedding)
        
        # 2. å…³é”®è¯æå–å’Œæ‰©å±•
        keywords = await self._extract_keywords(query)
        expanded_terms = await self._semantic_expansion(keywords)
        
        # 3. æŸ¥è¯¢æ„å›¾è¯†åˆ«
        intent = await self._classify_intent(query)
        
        return QueryAnalysis(
            domain=domain,
            keywords=expanded_terms, 
            intent=intent,
            embedding=embedding
        )
```

#### é˜¶æ®µ2ï¼šçŸ¥è¯†å¢å¼ºæ£€ç´¢
```python
class KnowledgeRetriever:
    async def retrieve(self, analysis: QueryAnalysis) -> KnowledgeBase:
        # å¹¶è¡Œæ£€ç´¢ç­–ç•¥
        search_tasks = [
            self._search_arxiv(analysis.keywords),
            self._search_openalex(analysis.domain),
            self._search_semantic_scholar(analysis.embedding)
        ]
        
        results = await asyncio.gather(*search_tasks, return_exceptions=True)
        
        # ç»“æœæ•´åˆ
        all_papers = self._consolidate_results(results)
        ranked_papers = self._rank_by_relevance(all_papers, analysis.embedding)
        
        return KnowledgeBase(
            papers=ranked_papers[:15],
            trends=await self._extract_trends(ranked_papers),
            gaps=await self._identify_gaps(ranked_papers)
        )
```

#### é˜¶æ®µ3ï¼šå¤šç­–ç•¥æƒ³æ³•ç”Ÿæˆ
```python
class IdeaGenerator:
    STRATEGIES = [
        "gap_based",      # ç ”ç©¶ç©ºç™½å¡«è¡¥
        "combination",    # æŠ€æœ¯ç»„åˆåˆ›æ–°  
        "extrapolation",  # è¶‹åŠ¿å¤–æ¨
        "cross_domain",   # è·¨é¢†åŸŸåº”ç”¨
        "improvement"     # ç°æœ‰æ–¹æ³•æ”¹è¿›
    ]
    
    async def generate(self, query: str, knowledge: KnowledgeBase) -> List[ResearchIdea]:
        ideas = []
        
        for strategy in self.STRATEGIES:
            strategy_ideas = await getattr(self, f"_generate_{strategy}_ideas")(
                query, knowledge
            )
            ideas.extend(strategy_ideas)
            
        return ideas
    
    async def _generate_gap_based_ideas(self, query: str, knowledge: KnowledgeBase) -> List[ResearchIdea]:
        # åŸºäºç ”ç©¶ç©ºç™½ç”Ÿæˆæƒ³æ³•
        prompt = self._build_gap_analysis_prompt(query, knowledge)
        response = await llm_service.chat(prompt, model="deepseek-reasoner")
        return self._parse_ideas_from_response(response)
```

#### é˜¶æ®µ4ï¼šç»“æ„åŒ–è¯„ä¼°ä¸ç­›é€‰
```python
class IdeaEvaluator:
    CRITERIA = {
        "novelty": {
            "weight": 0.3,
            "description": "æƒ³æ³•çš„åˆ›æ–°ç¨‹åº¦"
        },
        "feasibility": {
            "weight": 0.25, 
            "description": "æŠ€æœ¯å®ç°å¯è¡Œæ€§"
        },
        "impact": {
            "weight": 0.25,
            "description": "æ½œåœ¨å­¦æœ¯å½±å“åŠ›"
        },
        "clarity": {
            "weight": 0.2,
            "description": "æƒ³æ³•è¡¨è¾¾æ¸…æ™°åº¦"
        }
    }
    
    async def evaluate(self, ideas: List[ResearchIdea]) -> List[RatedIdea]:
        rated_ideas = []
        
        for idea in ideas:
            scores = {}
            explanations = {}
            
            for criterion, config in self.CRITERIA.items():
                score, explanation = await self._evaluate_criterion(idea, criterion)
                scores[criterion] = score
                explanations[criterion] = explanation
            
            # è®¡ç®—åŠ æƒæ€»åˆ†
            overall_score = sum(
                scores[c] * config["weight"] 
                for c, config in self.CRITERIA.items()
            )
            
            rated_ideas.append(RatedIdea(
                idea=idea,
                scores=scores,
                overall_score=overall_score,
                explanations=explanations
            ))
        
        return sorted(rated_ideas, key=lambda x: x.overall_score, reverse=True)
```

### æµå¼è¾“å‡ºè®¾è®¡
```python
async def stream_ideation_process(agent: IdeationAgent, query: str):
    try:
        # é˜¶æ®µæŒ‡ç¤º
        yield agent._format_thought("ğŸ” åˆ†ææ‚¨çš„ç ”ç©¶é¢†åŸŸå’Œéœ€æ±‚...")
        analysis = await agent.query_analyzer.analyze(query)
        
        yield agent._format_thought("ğŸ“š æ£€ç´¢ç›¸å…³æ–‡çŒ®å’Œç ”ç©¶è¶‹åŠ¿...")
        knowledge = await agent.knowledge_retriever.retrieve(analysis)
        
        yield agent._format_thought("ğŸ’¡ ç”Ÿæˆåˆ›æ–°ç ”ç©¶æƒ³æ³•...")
        ideas = await agent.idea_generator.generate(query, knowledge)
        
        yield agent._format_thought("ğŸ“Š è¯„ä¼°æƒ³æ³•è´¨é‡å’Œå¯è¡Œæ€§...")
        rated_ideas = await agent.idea_evaluator.evaluate(ideas)
        
        # æœ€ç»ˆè¾“å‡º
        yield agent._format_thought("âœ… ç”Ÿæˆæœ€ç»ˆç ”ç©¶ææ¡ˆ...")
        await agent._stream_final_output(rated_ideas)
        
    except asyncio.TimeoutError:
        yield agent._format_thought("â° æ—¶é—´é™åˆ¶å·²åˆ°ï¼Œè¾“å‡ºå½“å‰æœ€ä½³ç»“æœ...")
        await agent._stream_timeout_output()
    except Exception as e:
        yield agent._format_thought(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        await agent._fallback_generation(query)
```

---

## Paper Review èµ›é“å®ç°

### å†³ç­–å¾ªç¯è®¾è®¡

#### é˜¶æ®µ1ï¼šæ·±åº¦æ–‡æ¡£ç†è§£
```python
class PaperAnalyzer:
    async def analyze(self, pdf_content: str) -> PaperAnalysis:
        # 1. æ–‡æ¡£è§£æ
        text = document_processor.extract_text(pdf_content)
        chunks = document_processor.chunk_document(text)
        structure = document_processor.identify_structure(chunks)
        
        # 2. å¹¶è¡Œå†…å®¹åˆ†æ
        analysis_tasks = {
            "contributions": self._analyze_contributions(chunks, structure),
            "methodology": self._analyze_methodology(chunks, structure),
            "experiments": self._analyze_experiments(chunks, structure),
            "related_work": self._analyze_related_work(chunks, structure)
        }
        
        results = await asyncio.gather(*analysis_tasks.values())
        
        return PaperAnalysis(
            structure=structure,
            **dict(zip(analysis_tasks.keys(), results))
        )
    
    async def _analyze_contributions(self, chunks: List[DocumentChunk], structure: DocumentStructure) -> ContributionAnalysis:
        # ä½¿ç”¨æ¨ç†æ¨¡å‹æ·±åº¦åˆ†æåˆ›æ–°ç‚¹
        prompt = self._build_contribution_prompt(chunks, structure)
        return await llm_service.reason(prompt)
```

#### é˜¶æ®µ2ï¼šç›¸å…³ç ”ç©¶å¯¹æ¯”
```python
class RelatedWorkComparator:
    async def compare(self, paper_analysis: PaperAnalysis) -> List[Comparison]:
        # 1. æ£€ç´¢ç›¸å…³è®ºæ–‡
        query_embedding = await embedding_service.get_embedding(
            paper_analysis.contributions.summary
        )
        similar_papers = await academic_service.find_similar_papers(
            query_embedding, limit=10
        )
        
        # 2. æ·±åº¦å¯¹æ¯”åˆ†æ
        comparisons = []
        for paper in similar_papers:
            comparison = await self._compare_single_paper(paper_analysis, paper)
            comparisons.append(comparison)
        
        return sorted(comparisons, key=lambda x: x.similarity_score, reverse=True)
    
    async def _compare_single_paper(self, target: PaperAnalysis, other: Paper) -> Comparison:
        # å¤šç»´åº¦å¯¹æ¯”åˆ†æ
        comparison_tasks = {
            "novelty": self._compare_novelty(target, other),
            "methodology": self._compare_methodology(target, other),
            "results": self._compare_results(target, other)
        }
        
        results = await asyncio.gather(*comparison_tasks.values())
        
        return Comparison(
            paper=other,
            aspects=dict(zip(comparison_tasks.keys(), results)),
            similarity_score=await self._calculate_overall_similarity(target, other)
        )
```

#### é˜¶æ®µ3ï¼šç»“æ„åŒ–è¯„å®¡ç”Ÿæˆ
```python
class StructuredReviewGenerator:
    REQUIRED_SECTIONS = [
        "summary",
        "strengths", 
        "weaknesses",
        "questions",
        "scores"
    ]
    
    async def generate(self, paper_analysis: PaperAnalysis, comparisons: List[Comparison]) -> StructuredReview:
        # å¹¶è¡Œç”Ÿæˆå„è¯„å®¡éƒ¨åˆ†
        section_tasks = {
            "summary": self._generate_summary(paper_analysis),
            "strengths": self._generate_strengths(paper_analysis, comparisons),
            "weaknesses": self._generate_weaknesses(paper_analysis, comparisons),
            "questions": self._generate_questions(paper_analysis, comparisons)
        }
        
        section_results = await asyncio.gather(*section_tasks.values())
        
        # è®¡ç®—è¯„åˆ†
        scores = await self._calculate_scores(paper_analysis, comparisons)
        
        return StructuredReview(
            **dict(zip(section_tasks.keys(), section_results)),
            scores=scores
        )
    
    async def _calculate_scores(self, paper_analysis: PaperAnalysis, comparisons: List[Comparison]) -> ReviewScores:
        scoring_tasks = {
            "novelty": NoveltyScorer().score(paper_analysis, comparisons),
            "technical_quality": TechnicalQualityScorer().score(paper_analysis, comparisons),
            "clarity": ClarityScorer().score(paper_analysis, comparisons),
            "overall": OverallScorer().score(paper_analysis, comparisons),
            "confidence": ConfidenceScorer().score(paper_analysis, comparisons)
        }
        
        score_results = await asyncio.gather(*scoring_tasks.values())
        
        return ReviewScores(
            **dict(zip(scoring_tasks.keys(), score_results))
        )
```

### è¯„åˆ†ç³»ç»Ÿè®¾è®¡
```python
class BaseScorer(ABC):
    @abstractmethod
    async def score(self, paper_analysis: PaperAnalysis, comparisons: List[Comparison]) -> Score:
        pass

class NoveltyScorer(BaseScorer):
    async def score(self, paper_analysis: PaperAnalysis, comparisons: List[Comparison]) -> Score:
        # åˆ›æ–°æ€§è¯„åˆ†é€»è¾‘
        if not comparisons:
            return Score(value=7, explanation="æ— æ³•æ‰¾åˆ°è¶³å¤Ÿç›¸å…³ç ”ç©¶è¿›è¡Œå¯¹æ¯”")
        
        # åˆ†æä¸ç°æœ‰å·¥ä½œçš„å·®å¼‚åº¦
        novelty_analysis = await self._analyze_novelty(paper_analysis, comparisons)
        score_value = self._calculate_novelty_score(novelty_analysis)
        
        return Score(
            value=score_value,
            explanation=novelty_analysis.explanation,
            confidence=novelty_analysis.confidence
        )
```

### æµå¼è¾“å‡ºè®¾è®¡
```python
async def stream_review_process(agent: ReviewAgent, pdf_content: str, query: str):
    try:
        yield agent._format_thought("ğŸ“„ è§£æè®ºæ–‡å†…å®¹...")
        paper_analysis = await agent.paper_analyzer.analyze(pdf_content)
        
        yield agent._format_thought("ğŸ”¬ æŸ¥æ‰¾ç›¸å…³ç ”ç©¶è¿›è¡Œå¯¹æ¯”åˆ†æ...")
        comparisons = await agent.comparator.compare(paper_analysis)
        
        yield agent._format_thought("ğŸ“‹ ç”Ÿæˆç»“æ„åŒ–è¯„å®¡æ„è§...")
        review = await agent.review_generator.generate(paper_analysis, comparisons)
        
        yield agent._format_thought("âœ… éªŒè¯è¯„å®¡å®Œæ•´æ€§...")
        if await agent.validator.validate(review):
            await agent._stream_structured_review(review)
        else:
            yield agent._format_thought("âš ï¸ è¯„å®¡å†…å®¹ä¸å®Œæ•´ï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ...")
            await agent._fallback_review(paper_analysis)
            
    except asyncio.TimeoutError:
        yield agent._format_thought("â° æ—¶é—´é™åˆ¶å·²åˆ°ï¼Œè¾“å‡ºå½“å‰è¯„å®¡ç»“æœ...")
        await agent._stream_partial_review()
    except Exception as e:
        yield agent._format_thought(f"âŒ è¯„å®¡è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        await agent._fallback_basic_review(pdf_content)
```

---

## é…ç½®å’Œç¯å¢ƒç®¡ç†

### ç¯å¢ƒå˜é‡é…ç½®
```python
class Config:
    # æ¨¡å‹é…ç½®
    SCI_MODEL_BASE_URL: str = "https://api.deepseek.com"
    SCI_EMBEDDING_BASE_URL: str = "https://dashscope.aliyuncs.com/compatible-mode/v1"
    SCI_LLM_MODEL: str = "deepseek-chat"
    SCI_LLM_REASONING_MODEL: str = "deepseek-reasoner" 
    SCI_EMBEDDING_MODEL: str = "text-embedding-v4"
    
    # æœåŠ¡é…ç½®
    PORT: int = 3000
    LOG_LEVEL: str = "INFO"
    REQUEST_TIMEOUT: int = 900  # 15åˆ†é’Ÿ
    
    # æ€§èƒ½é…ç½®
    MAX_CONCURRENT_REQUESTS: int = 5
    EMBEDDING_CACHE_TTL: int = 3600
    API_RATE_LIMIT: int = 10  # è¯·æ±‚/ç§’
```

### Dockeré…ç½®
```dockerfile
FROM python:3.12-slim-bookworm

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 3000

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "3000"]
```

---

## é”™è¯¯å¤„ç†å’Œå¥å£®æ€§

### é”™è¯¯åˆ†ç±»å’Œå¤„ç†ç­–ç•¥
```python
class ErrorHandler:
    ERROR_CATEGORIES = {
        "api_timeout": {
            "level": "warning",
            "strategy": "retry_then_fallback",
            "max_retries": 2
        },
        "rate_limit": {
            "level": "warning", 
            "strategy": "wait_and_retry",
            "backoff_factor": 2.0
        },
        "parsing_error": {
            "level": "error",
            "strategy": "fallback_parsing",
            "degrade_gracefully": True
        },
        "model_error": {
            "level": "error",
            "strategy": "switch_model",
            "fallback_model": "deepseek-chat"
        }
    }
    
    async def handle_error(self, error: Exception, context: str) -> ErrorResponse:
        error_type = self._classify_error(error)
        strategy = self.ERROR_CATEGORIES.get(error_type, {})
        
        return await getattr(self, f"_handle_{strategy['strategy']}")(error, context)
```

### é™çº§ç­–ç•¥è®¾è®¡
```python
class FallbackStrategies:
    @staticmethod
    async def literature_review_fallback(query: str) -> str:
        """æ–‡çŒ®ç»¼è¿°é™çº§ç­–ç•¥"""
        # ä½¿ç”¨å†…ç½®çŸ¥è¯†åº“å’Œç®€åŒ–åˆ†æ
        return await simplified_review_generator.generate(query)
    
    @staticmethod 
    async def paper_review_fallback(pdf_content: str) -> str:
        """è®ºæ–‡è¯„å®¡é™çº§ç­–ç•¥"""
        # åŸºäºPDFæ–‡æœ¬çš„åŸºç¡€åˆ†æ
        text = document_processor.extract_text(pdf_content)
        return await basic_review_generator.generate(text)
    
    @staticmethod
    async def ideation_fallback(query: str) -> str:
        """æƒ³æ³•ç”Ÿæˆé™çº§ç­–ç•¥"""
        # ä½¿ç”¨é¢„å®šä¹‰æ¨¡æ¿å’Œæ¨¡å¼
        return await template_based_ideation.generate(query)
```

---

## æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### å¹¶å‘æ§åˆ¶
```python
class ConcurrentExecutor:
    def __init__(self, max_concurrent: int = 5):
        self.semaphore = asyncio.Semaphore(max_concurrent)
    
    async def execute_batch(self, tasks: List[Coroutine]) -> List[Any]:
        async with self.semaphore:
            return await asyncio.gather(*tasks, return_exceptions=True)
```

### ç¼“å­˜ç­–ç•¥
```python
class EmbeddingCache:
    def __init__(self, max_size: int = 1000, ttl: int = 3600):
        self.cache = TTLCache(maxsize=max_size, ttl=ttl)
    
    async def get_embedding(self, text: str) -> List[float]:
        cache_key = hashlib.md5(text.encode()).hexdigest()
        
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        embedding = await embedding_service.get_embedding(text)
        self.cache[cache_key] = embedding
        return embedding
```

### è¶…æ—¶ç®¡ç†
```python
class TimeoutManager:
    def __init__(self, total_timeout: int, stage_timeouts: Dict[str, int]):
        self.total_timeout = total_timeout
        self.stage_timeouts = stage_timeouts
        self.start_time = None
    
    async def execute_with_timeout(self, coro: Coroutine, stage: str) -> Any:
        stage_timeout = self.stage_timeouts.get(stage, self.total_timeout // 3)
        return await asyncio.wait_for(coro, timeout=stage_timeout)
```

---

## æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•è®¾è®¡
```python
class TestIdeationAgent:
    @pytest.mark.asyncio
    async def test_query_analysis(self):
        agent = IdeationAgent()
        analysis = await agent.query_analyzer.analyze("AI in healthcare")
        assert analysis.domain in DOMAINS
        assert len(analysis.keywords) > 0
    
    @pytest.mark.asyncio 
    async def test_idea_generation(self):
        agent = IdeationAgent()
        ideas = await agent.idea_generator.generate("test query", mock_knowledge_base)
        assert len(ideas) > 0
        assert all(hasattr(idea, 'title') for idea in ideas)
```

### é›†æˆæµ‹è¯•è®¾è®¡
```python
class TestEndToEnd:
    @pytest.mark.asyncio
    async def test_complete_ideation_flow(self):
        # æµ‹è¯•å®Œæ•´çš„ç ”ç©¶æ„æ€æµç¨‹
        async for chunk in ideation_agent.execute("AI in education"):
            assert isinstance(chunk, str)
            # éªŒè¯æµå¼è¾“å‡ºæ ¼å¼
    
    @pytest.mark.asyncio
    async def test_paper_review_with_sample_pdf(self):
        # æµ‹è¯•è®ºæ–‡è¯„å®¡æµç¨‹
        pdf_content = get_sample_pdf_base64()
        async for chunk in review_agent.execute(pdf_content, "è¯·è¯„å®¡è¿™ç¯‡è®ºæ–‡"):
            assert isinstance(chunk, str)
```

### æ€§èƒ½æµ‹è¯•
```python
class TestPerformance:
    def test_response_time(self):
        # æµ‹è¯•å“åº”æ—¶é—´æ˜¯å¦ç¬¦åˆè¶…æ—¶è¦æ±‚
        start_time = time.time()
        result = await agent.execute(test_query)
        elapsed = time.time() - start_time
        assert elapsed < MAX_TIMEOUT
    
    def test_concurrent_requests(self):
        # æµ‹è¯•å¹¶å‘å¤„ç†èƒ½åŠ›
        tasks = [agent.execute(f"query_{i}") for i in range(10)]
        results = await asyncio.gather(*tasks)
        assert len(results) == 10
```

---

## éƒ¨ç½²å’Œç»´æŠ¤

### ç›‘æ§å’Œæ—¥å¿—
```python
class Monitoring:
    @staticmethod
    def log_agent_operation(agent: str, operation: str, duration: float, success: bool):
        logger.info(
            f"Agent operation completed",
            extra={
                "agent": agent,
                "operation": operation, 
                "duration": duration,
                "success": success,
                "timestamp": datetime.utcnow().isoformat()
            }
        )
    
    @staticmethod
    def track_performance_metrics():
        # è·Ÿè¸ªå…³é”®æ€§èƒ½æŒ‡æ ‡
        metrics = {
            "response_times": [],
            "error_rates": [],
            "cache_hit_rates": []
        }
        return metrics
```

### å¥åº·æ£€æŸ¥
```python
@app.get("/health")
async def health_check():
    """ç»¼åˆå¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    checks = {
        "llm_service": await check_llm_health(),
        "embedding_service": await check_embedding_health(),
        "academic_apis": await check_academic_apis_health(),
        "memory_usage": get_memory_usage()
    }
    
    overall_status = "healthy" if all(checks.values()) else "degraded"
    
    return {
        "status": overall_status,
        "timestamp": datetime.utcnow().isoformat(),
        "checks": checks
    }
```

---

## å¼€å‘è·¯çº¿å›¾

### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€æ¡†æ¶ (2-3å¤©)
- [ ] é¡¹ç›®ç»“æ„å’Œé…ç½®ç®¡ç†
- [ ] åŸºç¡€ç»„ä»¶æ¡†æ¶ï¼ˆAgentBase, Configç­‰ï¼‰
- [ ] ç¯å¢ƒéƒ¨ç½²å’ŒDockeré…ç½®
- [ ] åŸºç¡€APIç«¯ç‚¹å®ç°

### ç¬¬äºŒé˜¶æ®µï¼šæ ¸å¿ƒæœåŠ¡ (3-4å¤©)  
- [ ] å­¦æœ¯æ•°æ®æ£€ç´¢æœåŠ¡
- [ ] æ–‡æ¡£å¤„ç†å¼•æ“
- [ ] åµŒå…¥å’Œç›¸ä¼¼åº¦æœåŠ¡
- [ ] ç¼“å­˜å’Œæ€§èƒ½ä¼˜åŒ–

### ç¬¬ä¸‰é˜¶æ®µï¼šæ™ºèƒ½ä½“å®ç° (4-5å¤©)
- [ ] Research Ideationæ™ºèƒ½ä½“
- [ ] Paper Reviewæ™ºèƒ½ä½“  
- [ ] å†³ç­–å¾ªç¯å’Œæµå¼è¾“å‡º
- [ ] é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥

### ç¬¬å››é˜¶æ®µï¼šæµ‹è¯•ä¼˜åŒ– (2-3å¤©)
- [ ] å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
- [ ] æ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ–
- [ ] å®‰å…¨æ€§å’Œåˆè§„æ€§æ£€æŸ¥
- [ ] æ–‡æ¡£å®Œå–„

### ç¬¬äº”é˜¶æ®µï¼šéƒ¨ç½²ä¸Šçº¿ (1-2å¤©)
- [ ] æœ€ç»ˆæµ‹è¯•å’ŒéªŒè¯
- [ ] ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- [ ] ç›‘æ§å’Œæ—¥å¿—é…ç½®
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•

---

## é£é™©ç®¡ç†å’Œåº”å¯¹ç­–ç•¥

### æŠ€æœ¯é£é™©
1. **APIæœåŠ¡ä¸ç¨³å®š**
   - åº”å¯¹ï¼šå¤šå±‚ç¼“å­˜ã€é‡è¯•æœºåˆ¶ã€é™çº§ç­–ç•¥
   
2. **å¤„ç†è¶…æ—¶**
   - åº”å¯¹ï¼šé˜¶æ®µè¶…æ—¶æ§åˆ¶ã€æ¸è¿›å¼è¾“å‡ºã€è¶…æ—¶é¢„å¤„ç†

3. **å†…å­˜ä½¿ç”¨è¿‡é«˜**
   - åº”å¯¹ï¼šæ–‡æ¡£åˆ†å—å¤„ç†ã€æµå¼å¤„ç†ã€å†…å­˜ç›‘æ§

### ä¸šåŠ¡é£é™©
1. **è¯„å®¡è´¨é‡ä¸ä¸€è‡´**
   - åº”å¯¹ï¼šç»“æ„åŒ–æ¨¡æ¿ã€è´¨é‡éªŒè¯ã€äººå·¥è¯„ä¼°æ ·æœ¬

2. **æƒ³æ³•åˆ›æ–°æ€§ä¸è¶³**  
   - åº”å¯¹ï¼šå¤šç­–ç•¥ç”Ÿæˆã€å¤–éƒ¨çŸ¥è¯†å¢å¼ºã€åˆ›æ–°æ€§è¯„ä¼°

### åˆè§„é£é™©
1. **ä½¿ç”¨æœªæˆæƒAPI**
   - åº”å¯¹ï¼šAPIç™½åå•æ£€æŸ¥ã€ä»£ç æ‰«æã€éƒ¨ç½²å‰éªŒè¯

2. **ç½‘ç»œè®¿é—®é—®é¢˜**
   - åº”å¯¹ï¼šä¸­å›½å¤§é™†ç½‘ç»œæµ‹è¯•ã€å¤‡ç”¨æ–¹æ¡ˆã€é”™è¯¯å¤„ç†

---

## æ€»ç»“

æœ¬æ–‡æ¡£æä¾›äº†å®Œæ•´çš„AI Scientist Challengeé¡¹ç›®å¼€å‘æŒ‡å—ï¼Œæ¶µç›–ä»æ¶æ„è®¾è®¡åˆ°å…·ä½“å®ç°çš„å„ä¸ªå±‚é¢ã€‚å…³é”®æˆåŠŸå› ç´ åŒ…æ‹¬ï¼š

1. **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¸…æ™°çš„ç»„ä»¶è¾¹ç•Œå’Œæ¥å£å®šä¹‰
2. **å†³ç­–å¾ªç¯**ï¼šæ™ºèƒ½çš„æ€è€ƒ-è¡ŒåŠ¨æµç¨‹æ§åˆ¶  
3. **å¥å£®æ€§**ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šåœ¨ä¸¥æ ¼çº¦æŸä¸‹çš„é«˜æ•ˆå®ç°
5. **åˆè§„æ€§**ï¼šå®Œå…¨éµå¾ªæ¯”èµ›è§„åˆ™å’ŒæŠ€æœ¯è¦æ±‚

å¼€å‘å›¢é˜Ÿåº”æŒ‰ç…§æœ¬æ–‡æ¡£çš„æŒ‡å¯¼è¿›è¡Œç³»ç»ŸåŒ–å¼€å‘ï¼Œç¡®ä¿æŒ‰æ—¶äº¤ä»˜é«˜è´¨é‡ã€é«˜æ€§èƒ½çš„æ™ºèƒ½ä½“ç³»ç»Ÿã€‚