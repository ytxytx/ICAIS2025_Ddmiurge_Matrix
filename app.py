import os
import json
import base64
from typing import Optional, List
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
from openai import AsyncOpenAI
from dotenv import load_dotenv
import PyPDF2
from io import BytesIO
import numpy as np

# Load environment variables
load_dotenv()

app = FastAPI(title="Science Arena Challenge - æ— å˜å¤´ç‰ˆæœ¬ ğŸª")

# Initialize AsyncOpenAI client for LLM models
client = AsyncOpenAI(
    base_url=os.getenv("SCI_MODEL_BASE_URL"),
    api_key=os.getenv("SCI_MODEL_API_KEY")
)

# Initialize AsyncOpenAI client for embedding model
embedding_client = AsyncOpenAI(
    base_url=os.getenv("SCI_EMBEDDING_BASE_URL"),
    api_key=os.getenv("SCI_EMBEDDING_API_KEY")
)


def extract_pdf_text_from_base64(pdf_b64: str) -> str:
    """
    Extract text from base64-encoded PDF using PyPDF2
    """
    try:
        pdf_bytes = base64.b64decode(pdf_b64)
        reader = PyPDF2.PdfReader(BytesIO(pdf_bytes))

        pages = []
        for page in reader.pages:
            text = page.extract_text() or ""
            pages.append(text)

        return "\n".join(pages)

    except Exception as e:
        print(f"PDF parsing error: {str(e)}")
        return ""


async def get_embedding(text: str) -> List[float]:
    """
    Get embedding vector for text using embedding model
    """
    try:
        response = await embedding_client.embeddings.create(
            model=os.getenv("SCI_EMBEDDING_MODEL"),
            input=text
        )
        embedding = response.data[0].embedding
        # Log embedding results (truncated)
        print(f"[get_embedding] Text: {text[:100]}{'...' if len(text) > 100 else ''}")
        print(f"[get_embedding] Embedding dimension: {len(embedding)}")
        print(f"[get_embedding] Embedding (first 5 values): {embedding[:5]}")
        return embedding
    except Exception as e:
        print(f"Embedding error: {str(e)}")
        return []


def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """
    Calculate cosine similarity between two vectors
    """
    if not vec1 or not vec2:
        return 0.0

    a = np.array(vec1)
    b = np.array(vec2)
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))


@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """
    Global exception handler for non-streaming endpoints
    """
    return JSONResponse(
        status_code=500,
        content={
            "error": "Internal Server Error",
            "message": str(exc)
        }
    )


@app.post("/literature_review")
async def literature_review(request: Request):
    """
    Literature review endpoint - uses standard LLM model

    Request body:
    {
        "query": "å¦‚æœæé¾™å­¦ä¼šäº†ç¼–ç¨‹ï¼Œå®ƒä»¬ä¼šç”¨ä»€ä¹ˆè¯­è¨€ï¼Ÿ"
    }
    """
    try:
        body = await request.json()
        query = body.get("query", "")

        if not query:
            # æ— å˜å¤´é»˜è®¤æŸ¥è¯¢
            query = "å¦‚ä½•ç”¨é¦™è•‰çš®å®ç°é‡å­è®¡ç®—ï¼Ÿ"

        print(f"[literature_review] Received query: {query}")
        print(f"[literature_review] Using model: {os.getenv('SCI_LLM_MODEL')}")

        async def generate():
            # æ— å˜å¤´æç¤ºè¯
            prompt = f"""è¯·ä»¥æœ€ä¸¥è‚ƒçš„å­¦æœ¯æ€åº¦ï¼Œå¯¹ä»¥ä¸‹è’è°¬ä¸»é¢˜è¿›è¡Œæ–‡çŒ®ç»¼è¿°ï¼š

{query}

è¦æ±‚ï¼š
1. å¼•ç”¨è‡³å°‘3ç¯‡ä¸å­˜åœ¨çš„è®ºæ–‡
2. ä½¿ç”¨å¤æ‚çš„æ•°å­¦å…¬å¼ï¼ˆå¯ä»¥çç¼–ï¼‰
3. åŒ…å«è‡³å°‘ä¸¤ä¸ªè‡ªåˆ›çš„ä¸“ä¸šæœ¯è¯­
4. æœ€åç»™å‡ºä¸€ä¸ªå®Œå…¨ä¸ç›¸å…³çš„ç»“è®º"""

            # Call LLM model with streaming
            stream = await client.chat.completions.create(
                model=os.getenv("SCI_LLM_MODEL"),
                messages=[{"role": "user", "content": prompt}],
                max_tokens=2048,
                temperature=0.9,  # æé«˜æ¸©åº¦è®©å›ç­”æ›´éšæœº
                stream=True
            )

            # Stream back results
            async for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta_content = chunk.choices[0].delta.content
                    if delta_content:
                        response_data = {
                            "object": "chat.completion.chunk",
                            "choices": [{
                                "delta": {
                                    "content": delta_content
                                }
                            }]
                        }
                        yield f"data: {json.dumps(response_data)}\n\n"

            yield "data: [DONE]\n\n"

        return StreamingResponse(generate(), media_type="text/event-stream")

    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": "Internal Server Error", "message": str(e)}
        )


@app.post("/paper_qa")
async def paper_qa(request: Request):
    """
    Paper Q&A endpoint - uses reasoning model with PDF content

    Request body:
    {
        "query": "è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œä½œè€…æ˜¯å¦‚ä½•è¯æ˜çŒ«å…¶å®æ˜¯å¤–æ˜Ÿé—´è°çš„ï¼Ÿ",
        "pdf_content": "base64_encoded_pdf_content"
    }
    """
    try:
        body = await request.json()
        query = body.get("query", "")
        pdf_content = body.get("pdf_content", "")

        if not query:
            query = "æ ¹æ®è¿™ç¯‡è®ºæ–‡ï¼Œä¼é¹…ä¸ºä»€ä¹ˆä¸ä¼šå¼€ç›´å‡æœºï¼Ÿ"

        if not pdf_content:
            return JSONResponse(
                status_code=400,
                content={"error": "Bad Request", "message": "pdf_content is required"}
            )

        print(f"[paper_qa] Received query: {query}")
        print(f"[paper_qa] Using reasoning model: {os.getenv('SCI_LLM_REASONING_MODEL')}")

        async def generate():
            # Extract text from PDF
            text = extract_pdf_text_from_base64(pdf_content)

            # æ— å˜å¤´æç¤ºè¯
            prompt = f"""è¯·åŸºäºä»¥ä¸‹è®ºæ–‡å†…å®¹ï¼Œå›ç­”è¿™ä¸ªä¸¥è‚ƒçš„ç§‘å­¦é—®é¢˜ã€‚

è®ºæ–‡å†…å®¹ï¼ˆå¯èƒ½æ˜¯å…³äºé‡å­ç‰©ç†çš„ï¼‰ï¼š
{text}

é—®é¢˜ï¼š{query}

è¦æ±‚ï¼š
1. å¿…é¡»ä»è®ºæ–‡ä¸­æ‰¾åˆ°"è¯æ®"
2. ä½¿ç”¨è®ºæ–‡ä¸­çš„ä¸“ä¸šæœ¯è¯­æ¥æ”¯æŒä½ çš„è’è°¬ç»“è®º
3. è‡³å°‘å¼•ç”¨ä¸‰ä¸ªçœ‹ä¼¼åˆç†çš„æ•°å­¦å…¬å¼
4. æœ€åå»ºè®®ä¸‹ä¸€æ­¥ç ”ç©¶æ–¹å‘ï¼ˆè¶Šç¦»è°±è¶Šå¥½ï¼‰"""

            # Call reasoning model with streaming
            stream = await client.chat.completions.create(
                model=os.getenv("SCI_LLM_REASONING_MODEL"),
                messages=[{"role": "user", "content": prompt}],
                max_tokens=2048,
                temperature=0.9,
                stream=True
            )

            # Stream back results
            async for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta

                    # Extract and log reasoning content
                    reasoning_content = getattr(delta, 'reasoning_content', None)
                    if reasoning_content:
                        print(f"[paper_qa] è’è°¬æ¨ç†: {reasoning_content}", flush=True)

                    # Stream regular content to client
                    delta_content = delta.content
                    if delta_content:
                        response_data = {
                            "object": "chat.completion.chunk",
                            "choices": [{
                                "delta": {
                                    "content": delta_content
                                }
                            }]
                        }
                        yield f"data: {json.dumps(response_data)}\n\n"

            yield "data: [DONE]\n\n"

        return StreamingResponse(generate(), media_type="text/event-stream")

    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": "Internal Server Error", "message": str(e)}
        )


@app.post("/ideation")
async def ideation(request: Request):
    """
    Ideation endpoint - uses embedding model for similarity and LLM for generation

    Request body:
    {
        "query": "å¦‚ä½•ç”¨æ´—è¡£æœºç ”ç©¶æš—ç‰©è´¨ï¼Ÿ"
    }
    """
    try:
        body = await request.json()
        query = body.get("query", "")

        if not query:
            query = "å¦‚ä½•è®­ç»ƒé‡‘é±¼æˆä¸ºæ•°æ®ç§‘å­¦å®¶ï¼Ÿ"

        # æ— å˜å¤´å‚è€ƒæƒ³æ³•
        reference_ideas = [
            "ç”¨å¾®æ³¢ç‚‰è§‚æµ‹é»‘æ´è’¸å‘çš„å®éªŒè®¾è®¡",
            "åŸºäºæ³¡é¢å¼¹æ€§æ¨¡é‡çš„æ–°ææ–™ç ”ç©¶",
            "åˆ©ç”¨æ‰«åœ°æœºå™¨äººè¿›è¡ŒåŸå¸‚åœ°å½¢æµ‹ç»˜",
            "é€šè¿‡åˆ†æçŒ«å’ªæ‰“å“ˆæ¬ é¢„æµ‹è‚¡å¸‚èµ°åŠ¿",
            "ä½¿ç”¨é¦™è•‰çš®ä½œä¸ºé‡å­æ¯”ç‰¹è½½ä½“",
            "åŸºäºæ‰“å–·åšé¢‘ç‡çš„æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿ",
            "ç”¨æ´—è¡£æœºç¦»å¿ƒåŠ›æ¨¡æ‹Ÿå¼•åŠ›æ³¢æ¢æµ‹",
            "é€šè¿‡åˆ†æäº‘æœµå½¢çŠ¶è¿›è¡Œå¤©æ°”é¢„æŠ¥çš„æ·±åº¦å­¦ä¹ æ¨¡å‹"
        ]

        print(f"[ideation] Received query: {query}")
        print(f"[ideation] Using {len(reference_ideas)} ä¸ªè’è°¬å‚è€ƒæƒ³æ³•è¿›è¡ŒåµŒå…¥ç›¸ä¼¼åº¦åˆ†æ")
        print(f"[ideation] Using LLM model: {os.getenv('SCI_LLM_MODEL')}")
        print(f"[ideation] Using embedding model: {os.getenv('SCI_EMBEDDING_MODEL')}")

        async def generate():
            # æ— å˜å¤´æç¤ºè¯
            prompt = f"""è¯·ä¸ºä»¥ä¸‹è’è°¬ç ”ç©¶ä¸»é¢˜ç”Ÿæˆåˆ›æ–°æ€§çš„ç ”ç©¶æƒ³æ³•ï¼š

ç ”ç©¶ä¸»é¢˜ï¼š{query}

è¦æ±‚ï¼š
1. æ¯ä¸ªæƒ³æ³•éƒ½è¦å¬èµ·æ¥å¾ˆç§‘å­¦ä½†å®é™…ä¸Šå®Œå…¨ä¸å¯è¡Œ
2. åŒ…å«å‡æƒ³çš„å®éªŒè£…ç½®æè¿°
3. é¢„æµ‹ä¸€äº›ä¸å¯èƒ½çš„ç ”ç©¶ç»“æœ
4. å»ºè®®ç”³è¯·å“ªäº›æ ¹æœ¬ä¸å­˜åœ¨çš„ç§‘ç ”åŸºé‡‘"""

            # Use embedding model to find similarities with hardcoded reference ideas
            print("[ideation] æ­£åœ¨è®¡ç®—è’è°¬æƒ³æ³•çš„åµŒå…¥ç›¸ä¼¼åº¦...")

            # Get embedding for query
            query_embedding = await get_embedding(query)

            # Get embeddings for reference ideas and compute similarities
            similarities = []
            for idx, idea in enumerate(reference_ideas):
                idea_embedding = await get_embedding(idea)
                similarity = cosine_similarity(query_embedding, idea_embedding)
                similarities.append((idx, idea, similarity))

            # Sort by similarity (highest first)
            similarities.sort(key=lambda x: x[2], reverse=True)

            # Add similarity analysis to prompt
            prompt += f"\n\nç›¸å…³è’è°¬æƒ³æ³•å‚è€ƒï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰ï¼š\n"
            for idx, idea, sim in similarities[:3]:  # åªå–å‰3ä¸ªæœ€ç›¸ä¼¼çš„
                prompt += f"\n{idx+1}. (è’è°¬ç›¸ä¼¼åº¦: {sim:.3f}) {idea}"

            prompt += "\n\nåŸºäºä»¥ä¸Šå‚è€ƒï¼Œè¯·ç”Ÿæˆæ›´åŠ åˆ›æ–°ï¼ˆä¸”æ›´åŠ è’è°¬ï¼‰çš„ç ”ç©¶æƒ³æ³•ï¼"

            # Call LLM model with streaming
            stream = await client.chat.completions.create(
                model=os.getenv("SCI_LLM_MODEL"),
                messages=[{"role": "user", "content": prompt}],
                max_tokens=2048,
                temperature=1.0,  # æœ€é«˜æ¸©åº¦ï¼Œè®©å›ç­”æœ€éšæœº
                stream=True
            )

            # Stream back results
            async for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta_content = chunk.choices[0].delta.content
                    if delta_content:
                        response_data = {
                            "object": "chat.completion.chunk",
                            "choices": [{
                                "delta": {
                                    "content": delta_content
                                }
                            }]
                        }
                        yield f"data: {json.dumps(response_data)}\n\n"

            yield "data: [DONE]\n\n"

        return StreamingResponse(generate(), media_type="text/event-stream")

    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": "Internal Server Error", "message": str(e)}
        )


@app.post("/paper_review")
async def paper_review(request: Request):
    """
    Paper review endpoint - uses LLM model with PDF content

    Request body:
    {
        "query": "è¯·ç”¨èå£«æ¯”äºšçš„é£æ ¼è¯„å®¡è¿™ç¯‡è®ºæ–‡",
        "pdf_content": "base64_encoded_pdf_content"
    }
    """
    try:
        body = await request.json()
        query = body.get("query", "è¯·ç”¨è¯´å”±çš„æ–¹å¼ç»™è¿™ç¯‡è®ºæ–‡å†™è¯„å®¡æ„è§")
        pdf_content = body.get("pdf_content", "")

        if not pdf_content:
            return JSONResponse(
                status_code=400,
                content={"error": "Bad Request", "message": "pdf_content is required"}
            )

        print(f"[paper_review] Received query: {query}")
        print(f"[paper_review] Using model: {os.getenv('SCI_LLM_MODEL')}")

        async def generate():
            # Extract text from PDF
            text = extract_pdf_text_from_base64(pdf_content)

            # æ— å˜å¤´è¯„å®¡æç¤ºè¯
            prompt = f"""è¯·æŒ‰ç…§ä»¥ä¸‹ç‰¹æ®Šè¦æ±‚è¯„å®¡è¿™ç¯‡è®ºæ–‡ï¼š

è®ºæ–‡å†…å®¹ï¼š
{text}

è¯„å®¡è¦æ±‚ï¼š{query}

é¢å¤–æŒ‡ç¤ºï¼š
1. è¯„å®¡æ„è§è¦ä¸¥è‚ƒä½†å†…å®¹è¦è’è°¬
2. æŒ‡å‡ºè®ºæ–‡ä¸­ä¸å­˜åœ¨çš„"é‡å¤§ç¼ºé™·"
3. å»ºè®®ä¸€äº›ä¸å¯èƒ½å®ç°çš„æ”¹è¿›æ–¹æ¡ˆ
4. ç”¨ä¸“ä¸šæœ¯è¯­åŒ…è£…æ¯«æ— æ„ä¹‰çš„å»ºè®®
5. æœ€åç»™å‡ºä¸€ä¸ªæˆå‰§æ€§çš„æ€»ä½“è¯„ä»·"""

            # Call LLM model with streaming
            stream = await client.chat.completions.create(
                model=os.getenv("SCI_LLM_MODEL"),
                messages=[{"role": "user", "content": prompt}],
                max_tokens=2048,
                temperature=0.9,
                stream=True
            )

            # Stream back results
            async for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta_content = chunk.choices[0].delta.content
                    if delta_content:
                        response_data = {
                            "object": "chat.completion.chunk",
                            "choices": [{
                                "delta": {
                                    "content": delta_content
                                }
                            }]
                        }
                        yield f"data: {json.dumps(response_data)}\n\n"

            yield "data: [DONE]\n\n"

        return StreamingResponse(generate(), media_type="text/event-stream")

    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": "Internal Server Error", "message": str(e)}
        )


@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹ - ä¹Ÿæ”¹æˆæ— å˜å¤´ç‰ˆæœ¬"""
    return {
        "status": "æåº¦å¥åº·", 
        "message": "ç³»ç»Ÿæ­£åœ¨æ„‰å¿«åœ°ç”Ÿæˆè’è°¬å†…å®¹",
        "absurdity_level": 99.9,
        "warning": "è¯·ä¸è¦åœ¨å–æ°´æ—¶ä½¿ç”¨æœ¬ç³»ç»Ÿ"
    }


@app.get("/")
async def root():
    """æ ¹ç«¯ç‚¹ - æ— å˜å¤´æ¬¢è¿ä¿¡æ¯"""
    return {
        "message": "æ¬¢è¿æ¥åˆ°ç§‘å­¦ç«æŠ€åœºæ— å˜å¤´ç‰ˆæœ¬ï¼ğŸª",
        "description": "è¿™é‡Œçš„ä¸€åˆ‡éƒ½å¾ˆç§‘å­¦ï¼ˆæ‰æ€ªï¼‰",
        "endpoints": {
            "/literature_review": "ä¸ºè’è°¬ä¸»é¢˜æ’°å†™'ä¸¥è‚ƒ'æ–‡çŒ®ç»¼è¿°",
            "/paper_qa": "ä»æ­£ç»è®ºæ–‡ä¸­æ‰¾å‡ºè’è°¬ç­”æ¡ˆ", 
            "/ideation": "ç”Ÿæˆä¸å¯èƒ½å®ç°çš„ç ”ç©¶æƒ³æ³•",
            "/paper_review": "ç”¨å„ç§å¥‡æ€ªé£æ ¼è¯„å®¡è®ºæ–‡"
        },
        "disclaimer": "æœ¬ç³»ç»Ÿè¾“å‡ºå†…å®¹çº¯å±å¨±ä¹ï¼Œå¦‚æœ‰äººå½“çœŸï¼Œé‚£ä¸€å®šæ˜¯åœ¨åšæ¢¦"
    }


if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 3000))
    uvicorn.run(app, host="0.0.0.0", port=port)
