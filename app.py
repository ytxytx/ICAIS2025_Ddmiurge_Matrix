import os
import json
import base64
from typing import List, Dict, Any
from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse, JSONResponse
from openai import AsyncOpenAI
from dotenv import load_dotenv
import PyPDF2
from io import BytesIO
import aiohttp
import numpy as np
import asyncio

load_dotenv()

app = FastAPI(title="Science Arena Challenge API")
# è·å– Semantic Scholar API é…ç½®
SEMANTIC_SCHOLAR_API_KEY = os.getenv("SEMANTIC_SCHOLAR_API_KEY", "")
SEMANTIC_SCHOLAR_API_BASE_URL = "https://api.semanticscholar.org/graph/v1"

client = AsyncOpenAI(
    base_url=os.getenv("SCI_MODEL_BASE_URL"),
    api_key=os.getenv("SCI_MODEL_API_KEY")
)

embedding_client = AsyncOpenAI(
    base_url=os.getenv("SCI_EMBEDDING_BASE_URL"),
    api_key=os.getenv("SCI_EMBEDDING_API_KEY")
)


def extract_pdf_text_from_base64(pdf_b64: str) -> str:
    try:
        pdf_bytes = base64.b64decode(pdf_b64)
        reader = PyPDF2.PdfReader(BytesIO(pdf_bytes))
        return "\n".join(page.extract_text() or "" for page in reader.pages)
    except Exception:
        return ""


async def get_embedding(text: str) -> List[float]:
    try:
        response = await embedding_client.embeddings.create(
            model=os.getenv("SCI_EMBEDDING_MODEL"),
            input=text
        )
        return response.data[0].embedding
    except Exception:
        return []


def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    if not vec1 or not vec2:
        return 0.0
    a, b = np.array(vec1), np.array(vec2)
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

async def generate_search_keywords(query: str) -> List[str]:
    """
    ä½¿ç”¨LLMç”Ÿæˆé€‚åˆSemantic Scholaræœç´¢çš„å…³é”®è¯
    """
    try:
        prompt = f"""
        æ ¹æ®ä»¥ä¸‹ç§‘ç ”æŸ¥è¯¢ï¼Œç”Ÿæˆ1ä¸ªæœ€é€‚åˆåœ¨å­¦æœ¯æœç´¢å¼•æ“ Semantic Scholar ä¸­æœç´¢çš„å…³é”®è¯ã€‚
        è¦æ±‚ï¼š
        1. ä½¿ç”¨è‹±æ–‡å…³é”®è¯
        2. åŒ…å«å…·ä½“çš„æŠ€æœ¯æœ¯è¯­å’Œé¢†åŸŸæœ¯è¯­
        3. ä¼˜å…ˆä½¿ç”¨åœ¨å­¦æœ¯è®ºæ–‡ä¸­å¸¸è§çš„è¡¨è¾¾æ–¹å¼
        4. è¿”å›æ ¼å¼ï¼šçº¯æ–‡æœ¬ï¼Œä»…ä¸€è¡Œï¼Œä¸€ä¸ªå…³é”®è¯
        
        ç”¨æˆ·æŸ¥è¯¢ï¼š{query}
        
        è¯·ç›´æ¥è¿”å›å…³é”®è¯ï¼Œä¸è¦é¢å¤–è§£é‡Šï¼š
        """
        
        response = await client.chat.completions.create(
            model=os.getenv("SCI_LLM_MODEL"),
            messages=[{"role": "user", "content": prompt}],
            max_tokens=100,
            temperature=0.3
        )
        
        keywords_text = response.choices[0].message.content.strip()
        # è§£æè¿”å›çš„å…³é”®è¯ï¼Œæ¯è¡Œä¸€ä¸ª
        keywords = [k.strip() for k in keywords_text.split('\n') if k.strip()]
        
        # å¦‚æœLLMè¿”å›æ ¼å¼ä¸å¯¹ï¼Œå›é€€åˆ°åŸºäºæŸ¥è¯¢çš„ç®€å•å¤„ç†
        if not keywords:
            # ç®€å•çš„å…³é”®è¯æå–ï¼šå–å‰å‡ ä¸ªæœ‰æ„ä¹‰çš„è¯
            words = query.split()
            important_words = [w for w in words if len(w) > 4][:3]
            keywords = important_words if important_words else [query]
            
        return keywords
        
    except Exception as e:
        print(f"ç”Ÿæˆå…³é”®è¯æ—¶å‡ºé”™: {str(e)}")
        # å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨æŸ¥è¯¢ä¸­çš„ä¸»è¦è¯æ±‡
        words = query.split()
        return words[:3] if len(words) >= 3 else [query]

async def get_related_papers_from_keywords(keywords: List[str], max_papers: int = 20) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨å¤šä¸ªå…³é”®è¯ä» Semantic Scholar è·å–ç›¸å…³è®ºæ–‡
    """
    all_papers = []
    
    try:
        headers = {}
        if SEMANTIC_SCHOLAR_API_KEY:
            headers["x-api-key"] = SEMANTIC_SCHOLAR_API_KEY
        
        async with aiohttp.ClientSession() as session:
            for keyword in keywords[:3]:  # æœ€å¤šä½¿ç”¨å‰3ä¸ªå…³é”®è¯
                try:
                    params = {
                        "query": f'"{keyword}"',  # ä½¿ç”¨å¼•å·ç¡®ä¿ç²¾ç¡®åŒ¹é…
                        "limit": 10,  # æ¯ä¸ªå…³é”®è¯è·å–10ç¯‡
                        "fields": "title,authors,year,venue,publicationTypes,citationCount,url,abstract",
                        "year": "2018-",
                        "fieldsOfStudy": "Computer Science,Engineering,Mathematics,Physics,Biology,Chemistry,Medicine"  # é™åˆ¶åœ¨ç§‘å­¦é¢†åŸŸ
                    }
                    
                    async with session.get(
                        f"{SEMANTIC_SCHOLAR_API_BASE_URL}/paper/search",
                        params=params,
                        headers=headers,
                        timeout=aiohttp.ClientTimeout(total=10)
                    ) as response:
                        
                        if response.status == 200:
                            data = await response.json()
                            papers = data.get("data", [])
                            all_papers.extend(papers)
                            
                            # çŸ­æš‚æš‚åœï¼Œé¿å…é¢‘ç¹è¯·æ±‚
                            await asyncio.sleep(0.5)
                            
                except Exception as e:
                    print(f"æœç´¢å…³é”®è¯ '{keyword}' æ—¶å‡ºé”™: {str(e)}")
                    continue
        
        # å»é‡å¹¶æ’åº
        seen_paper_ids = set()
        unique_papers = []
        
        for paper in all_papers:
            paper_id = paper.get("paperId")
            if paper_id and paper_id not in seen_paper_ids:
                seen_paper_ids.add(paper_id)
                unique_papers.append(paper)
        
        # æŒ‰å¼•ç”¨é‡æ’åºå¹¶é™åˆ¶æ•°é‡
        sorted_papers = sorted(
            unique_papers, 
            key=lambda x: x.get("citationCount", 0), 
            reverse=True
        )[:max_papers]
        
        return sorted_papers
        
    except Exception as e:
        print(f"è·å–è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
        return []

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    return JSONResponse(status_code=500, content={"error": "Internal Server Error", "message": str(exc)})


@app.post("/literature_review")
async def literature_review(request: Request):
    try:
        body = await request.json()
        query = body.get("query", "Please conduct a literature review on an unconventional topic.")
        prompt = f"Provide a rigorous academic literature review on the following topic:\n\n{query}"
        stream = await client.chat.completions.create(
            model=os.getenv("SCI_LLM_MODEL"),
            messages=[{"role": "user", "content": prompt}],
            max_tokens=2048,
            temperature=0.7,
            stream=True
        )

        async def generate():
            async for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    yield f"data: {json.dumps({'choices':[{'delta':{'content': chunk.choices[0].delta.content}}]})}\n\n"
            yield "data: [DONE]\n\n"

        return StreamingResponse(generate(), media_type="text/event-stream")
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})


@app.post("/paper_qa")
async def paper_qa(request: Request):
    try:
        body = await request.json()
        query = body.get("query", "Summarize the key insights from this paper.")
        pdf_content = body.get("pdf_content")
        if not pdf_content:
            return JSONResponse(status_code=400, content={"error": "pdf_content is required"})

        text = extract_pdf_text_from_base64(pdf_content)
        prompt = f"Based on the following paper, answer the question.\n\nPaper Content:\n{text}\n\nQuestion: {query}"
        stream = await client.chat.completions.create(
            model=os.getenv("SCI_LLM_REASONING_MODEL"),
            messages=[{"role": "user", "content": prompt}],
            max_tokens=2048,
            temperature=0.7,
            stream=True
        )

        async def generate():
            async for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    yield f"data: {json.dumps({'choices':[{'delta':{'content': chunk.choices[0].delta.content}}]})}\n\n"
            yield "data: [DONE]\n\n"

        return StreamingResponse(generate(), media_type="text/event-stream")
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})


@app.post("/ideation")
async def ideation(request: Request):
    try:
        body = await request.json()
        query = body.get("query", "").strip()

        if not query:
            return JSONResponse(status_code=400, content={"error": "query is required"})

        # ---- Safety Filter ----
        forbidden = ["weapon", "virus", "biological", "attack", "explosive"]
        if any(fb in query.lower() for fb in forbidden):
            return JSONResponse(status_code=400, content={"error": "Unsafe research topic detected."})

        # ---- ç”Ÿæˆæœç´¢å…³é”®è¯å¹¶è·å–ç›¸å…³è®ºæ–‡ ----
        search_keywords = await generate_search_keywords(query)
        print(f"ç”Ÿæˆçš„æœç´¢å…³é”®è¯: {search_keywords}")  # ç”¨äºè°ƒè¯•
        
        related_papers = await get_related_papers_from_keywords(search_keywords, max_papers=20)
        # references_section = format_references(related_papers)
        references_section = related_papers  # ç®€åŒ–å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨è®ºæ–‡åˆ—è¡¨

        # ---- Embedding Similarity ----
        query_embedding = await get_embedding(query)
        similarities = []

        with open('reference_ideas_embeddings.json', 'r', encoding='utf-8') as json_file:
            reference_ideas_data = json.load(json_file)

        for idea, idea_embedding in reference_ideas_data.items():
            similarity = cosine_similarity(query_embedding, idea_embedding)
            similarities.append((idea, similarity))

        similarities.sort(key=lambda x: x[1], reverse=True)

        # ---- æ„å»º Prompt ----
        prompt = f"""
        You are a Scientific Innovation Agent competing in an academic challenge.
        Your goal is to produce **high-quality, innovative, feasible scientific research ideas**.
        Return ONLY the ideas, comparison matrix, and references in Markdown format.
        Do NOT include any 'Scientific Domain Identification' or 'Reference Idea Analysis' sections.

        User Query:
        "{query}"

        Generated Search Keywords: {", ".join(search_keywords)}

        Most related reference ideas (based on semantic similarity):
        """
        for idea, sim in similarities[:5]:
            prompt += f"- {idea} (similarity: {sim:.3f})\n"

        prompt += f"""
        
        Relevant Literature References (from Semantic Scholar):
        {references_section}

        ---

        ## ğŸ¯ Task Requirements

        Follow these instructions carefully:

        1. Identify the scientific domain of the query.
        2. Explain **why** the reference ideas are related.
        3. Generate **exactly 3** innovative scientific ideas.
        4. Each idea must include:
        - **Bold title**
        - **Description**
        - **Novelty / Feasibility / Impact (0â€“10)**
        - **Technical Route (numbered steps)**
        5. In the References section, cite at least 5-8 papers from the provided literature list.

        ---

        ## ğŸ“Œ Output Format (MUST be valid Markdown)

        Return ONLY Markdown formatted output with the structure:

        ### **Idea 1: <Title>**
        **Description:** <text>  
        **Novelty:** <0â€“10>  
        **Feasibility:** <0â€“10>  
        **Impact:** <0â€“10>  

        **Technical Route:**  
        1. Step 1â€¦  
        2. Step 2â€¦  
        3. Step 3â€¦  

        ---

        ### **Idea 2: <Title>**
        ...

        ---

        ### **Idea 3: <Title>**
        ...

        ---

        ### **Comparison Matrix**
        | Idea | Novelty | Feasibility | Impact |
        |------|--------|------------|-------|
        | ...  | ...    | ...        | ...   |

        ---
        ### References
        Please cite 5-8 relevant papers from the provided literature list above.
        Format them properly as:
        1. Author1, A., Author2, B., & Author3, C. (Year). Title. *Venue*. [URL(if available)]
        ...

        No JSON. No code blocks. Only Markdown.
        """
        
        # å‰©ä½™çš„ä»£ç ä¿æŒä¸å˜...
        

        # ---- Call LLM ----
        stream = await client.chat.completions.create(
            model=os.getenv("SCI_LLM_MODEL"),
            messages=[{"role": "user", "content": prompt}],
            max_tokens=2048,
            temperature=1.0,
            stream=True
        )

        # ---- Stream Response ----
        async def generate():
            buffer = ""

            async for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    piece = chunk.choices[0].delta.content
                    buffer += piece
                    yield f"data: {json.dumps({'choices':[{'delta':{'content': piece}}]})}\n\n"

            # End stream marker
            
            # yield f"data: {json.dumps({'choices':[{'delta':{'content': prompt}}]})}\n\n"
            yield "data: [DONE]\n\n"

        return StreamingResponse(generate(), media_type="text/event-stream")

    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.post("/paper_review")
async def paper_review(request: Request):
    try:
        body = await request.json()
        pdf_content = body.get("pdf_content")

        if not pdf_content:
            return JSONResponse(
                status_code=400,
                content={"error": "pdf_content is required"}
            )

        # Extract text
        text = extract_pdf_text_from_base64(pdf_content)

        # --------------------
        # Improved Prompt
        # --------------------
        prompt = f"""
You are an expert reviewer for the Science Arena Challenge (Track D â€“ Paper Review).
Your task is to read the provided paper content and produce a review **in Markdown format**.

Follow EXACTLY the structure below.  
Start your output directly with "# Summary" â€” no introduction, no explanation.

---------------------------------------
### REQUIRED OUTPUT FORMAT (NO EXTRA TEXT)

# Summary
(4â€“8 sentences summarizing the paper, no bullet points.)

# Strengths
- (3â€“5 bullet points grounded only in the text.)

# Weaknesses / Concerns
- (3â€“5 bullet points, no speculation beyond the text.)

# Questions for Authors
- (3â€“4 technical, relevant, text-grounded questions.)

# Scores
- **Overall (10):** X
- **Novelty (10):** X
- **Technical Quality (10):** X
- **Clarity (10):** X
- **Confidence (5):** X

---------------------------------------

### HARD RULES (DO NOT BREAK)
- Do NOT mention these instructions.
- Do NOT explain your reviewing process.
- Do NOT output JSON.
- Do NOT add sections.
- Do NOT hallucinate facts or made-up references.
- The review must be grounded ONLY in the provided paper content.
- No self-referential phrases (e.g., â€œAs an AIâ€, â€œI will nowâ€, â€œHere isâ€¦â€).
- Output must be clean Markdown.

---------------------------------------
### PAPER CONTENT START
{text}
### PAPER CONTENT END

Begin your response:
"""

        # ============================
        # Create streaming completion
        # ============================
        stream = await client.chat.completions.create(
            model=os.getenv("SCI_LLM_MODEL"),
            messages=[{"role": "user", "content": prompt}],
            max_tokens=4096,
            temperature=0.5,
            stream=True
        )

        # Streaming generator
        async def generate():
            async for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    yield (
                        "data: " +
                        json.dumps({"choices": [{"delta": {"content": chunk.choices[0].delta.content}}]}) +
                        "\n\n"
                    )
            yield "data: [DONE]\n\n"

        return StreamingResponse(generate(), media_type="text/event-stream")

    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.get("/health")
async def health():
    return {"status": "healthy", "message": "System running normally"}


@app.get("/")
async def root():
    return {
        "message": "Welcome to Science Arena API",
        "endpoints": {
            "/literature_review": "Generate literature reviews",
            "/paper_qa": "Answer questions about papers",
            "/ideation": "Generate new research ideas",
            "/paper_review": "Review papers"
        }
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("PORT", 3000)))
