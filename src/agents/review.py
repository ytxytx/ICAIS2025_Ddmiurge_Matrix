import asyncio
import logging
from typing import AsyncGenerator, List, Dict, Any
from datetime import datetime

from .base import AgentBase
from ..models import (
    AgentConfig, PaperAnalysis, Comparison, StructuredReview, 
    ReviewScores, ThoughtStep
)
from ..services.document_processor import DocumentProcessor
from ..services.academic_data import AcademicDataService
from ..services.embedding_service import EmbeddingService


class ReviewAgent(AgentBase):
    """è®ºæ–‡è¯„å®¡æ™ºèƒ½ä½“ - å¯¹è®ºæ–‡è¿›è¡Œç»“æ„åŒ–è¯„å®¡"""
    
    def __init__(self, config: AgentConfig):
        super().__init__(config)
        self.document_processor = DocumentProcessor()
        self.academic_service = AcademicDataService()
        self.embedding_service = EmbeddingService()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.paper_analyzer = PaperAnalyzer(self.document_processor)
        self.comparator = RelatedWorkComparator(self.academic_service, self.embedding_service)
        self.review_generator = StructuredReviewGenerator(self.client)
        self.validator = ReviewValidator()

    async def execute(self, query: str, **kwargs) -> AsyncGenerator[str, None]:
        """æ‰§è¡Œè®ºæ–‡è¯„å®¡ä»»åŠ¡"""
        pdf_content = kwargs.get('pdf_content', '')
        
        if not self._validate_input(query) or not pdf_content:
            yield self._format_thought("âŒ æŸ¥è¯¢å†…å®¹æˆ–PDFæ–‡ä»¶æ— æ•ˆ")
            return

        try:
            # æ€è€ƒå¾ªç¯
            async for thought in self._thinking_loop():
                yield thought

            # æ‰§è¡Œè¯„å®¡æµç¨‹
            async for chunk in self._review_process(query, pdf_content):
                yield chunk
                    
        except asyncio.TimeoutError:
            self.logger.warning("è®ºæ–‡è¯„å®¡ä»»åŠ¡è¶…æ—¶")
            async for chunk in self._handle_timeout():
                yield chunk
        except Exception as e:
            self.logger.error(f"è®ºæ–‡è¯„å®¡ä»»åŠ¡å¤±è´¥: {str(e)}")
            async for chunk in self._fallback_strategy(query, pdf_content):
                yield chunk

    async def _thinking_loop(self) -> AsyncGenerator[str, None]:
        """è®ºæ–‡è¯„å®¡æ€è€ƒå¾ªç¯"""
        steps = []
        
        # æ–‡æ¡£è§£æé˜¶æ®µ
        analysis_step = ThoughtStep(
            step_type="analysis",
            content="è§£æè®ºæ–‡å†…å®¹å’Œç»“æ„...",
            timestamp=datetime.now()
        )
        steps.append(analysis_step)
        yield self._format_thought("ğŸ“„ è§£æè®ºæ–‡å†…å®¹å’Œç»“æ„...")
        await asyncio.sleep(0.5)
        
        # ç›¸å…³ç ”ç©¶å¯¹æ¯”é˜¶æ®µ
        comparison_step = ThoughtStep(
            step_type="comparison",
            content="æŸ¥æ‰¾ç›¸å…³ç ”ç©¶è¿›è¡Œå¯¹æ¯”åˆ†æ...",
            timestamp=datetime.now()
        )
        steps.append(comparison_step)
        yield self._format_thought("ğŸ”¬ æŸ¥æ‰¾ç›¸å…³ç ”ç©¶è¿›è¡Œå¯¹æ¯”åˆ†æ...")
        await asyncio.sleep(0.5)
        
        # è¯„å®¡ç”Ÿæˆé˜¶æ®µ
        review_step = ThoughtStep(
            step_type="review",
            content="ç”Ÿæˆç»“æ„åŒ–è¯„å®¡æ„è§...",
            timestamp=datetime.now()
        )
        steps.append(review_step)
        yield self._format_thought("ğŸ“‹ ç”Ÿæˆç»“æ„åŒ–è¯„å®¡æ„è§...")
        await asyncio.sleep(0.5)
        
        # éªŒè¯é˜¶æ®µ
        validation_step = ThoughtStep(
            step_type="validation",
            content="éªŒè¯è¯„å®¡å®Œæ•´æ€§å’Œè´¨é‡...",
            timestamp=datetime.now()
        )
        steps.append(validation_step)
        yield self._format_thought("âœ… éªŒè¯è¯„å®¡å®Œæ•´æ€§å’Œè´¨é‡...")
        
        self.thought_steps = steps

    async def _review_process(self, query: str, pdf_content: str) -> AsyncGenerator[str, None]:
        """è®ºæ–‡è¯„å®¡æµç¨‹"""
        try:
            # 1. è®ºæ–‡åˆ†æ
            yield self._format_thought("ğŸ“„ æ·±åº¦è§£æè®ºæ–‡å†…å®¹...")
            paper_analysis = await self.paper_analyzer.analyze(pdf_content)
            
            # 2. ç›¸å…³ç ”ç©¶å¯¹æ¯”
            yield self._format_thought("ğŸ”¬ æ£€ç´¢å’Œå¯¹æ¯”ç›¸å…³ç ”ç©¶å·¥ä½œ...")
            comparisons = await self.comparator.compare(paper_analysis)
            
            # 3. ç”Ÿæˆç»“æ„åŒ–è¯„å®¡
            yield self._format_thought("ğŸ“‹ ç”Ÿæˆè¯¦ç»†çš„ç»“æ„åŒ–è¯„å®¡...")
            review = await self.review_generator.generate(paper_analysis, comparisons, query)
            
            # 4. éªŒè¯è¯„å®¡è´¨é‡
            yield self._format_thought("âœ… éªŒè¯è¯„å®¡å†…å®¹çš„å®Œæ•´æ€§...")
            if await self.validator.validate(review):
                await self._stream_structured_review(review)
            else:
                yield self._format_thought("âš ï¸ è¯„å®¡å†…å®¹ä¸å®Œæ•´ï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ...")
                await self._fallback_review(paper_analysis)
            
        except Exception as e:
            self.logger.error(f"è®ºæ–‡è¯„å®¡æµç¨‹å¤±è´¥: {str(e)}")
            yield self._format_thought(f"âŒ è®ºæ–‡è¯„å®¡è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            async for chunk in self._fallback_basic_review(pdf_content):
                yield chunk

    async def _stream_structured_review(self, review: StructuredReview):
        """æµå¼è¾“å‡ºç»“æ„åŒ–è¯„å®¡"""
        # è¾“å‡ºæ‘˜è¦
        summary_output = f"## è®ºæ–‡æ‘˜è¦\n\n{review.summary}\n\n"
        async for chunk in self._stream_llm(summary_output):
            yield chunk
        
        # è¾“å‡ºä¼˜ç‚¹
        strengths_output = f"## è®ºæ–‡ä¼˜ç‚¹\n\n" + "\n".join([f"- {strength}" for strength in review.strengths]) + "\n\n"
        async for chunk in self._stream_llm(strengths_output):
            yield chunk
        
        # è¾“å‡ºç¼ºç‚¹
        weaknesses_output = f"## è®ºæ–‡ä¸è¶³\n\n" + "\n".join([f"- {weakness}" for weakness in review.weaknesses]) + "\n\n"
        async for chunk in self._stream_llm(weaknesses_output):
            yield chunk
        
        # è¾“å‡ºé—®é¢˜
        questions_output = f"## ä½œè€…é—®é¢˜\n\n" + "\n".join([f"- {question}" for question in review.questions]) + "\n\n"
        async for chunk in self._stream_llm(questions_output):
            yield chunk
        
        # è¾“å‡ºè¯„åˆ†
        scores_output = f"## è¯„åˆ†ç»“æœ\n\n"
        scores_output += f"- **æ€»ä½“è¯„åˆ†**: {review.scores.overall:.1f}/10\n"
        scores_output += f"- **åˆ›æ–°æ€§**: {review.scores.novelty:.1f}/10\n"
        scores_output += f"- **æŠ€æœ¯è´¨é‡**: {review.scores.technical_quality:.1f}/10\n"
        scores_output += f"- **æ¸…æ™°åº¦**: {review.scores.clarity:.1f}/10\n"
        scores_output += f"- **è¯„å®¡ä¿¡å¿ƒ**: {review.scores.confidence:.1f}/5\n\n"
        
        async for chunk in self._stream_llm(scores_output):
            yield chunk

    async def _fallback_review(self, paper_analysis: PaperAnalysis):
        """è¯„å®¡é™çº§ç­–ç•¥"""
        yield self._format_thought("âš ï¸ ä½¿ç”¨ç®€åŒ–è¯„å®¡æ¨¡å¼...")
        
        prompt = f"""
        è¯·åŸºäºä»¥ä¸‹è®ºæ–‡åˆ†æç»“æœæä¾›ç®€è¦è¯„å®¡ï¼š

        è®ºæ–‡æ ‡é¢˜ï¼š{paper_analysis.structure.title or 'æœªè¯†åˆ«'}
        è®ºæ–‡æ‘˜è¦ï¼š{paper_analysis.structure.abstract or 'æœªæå–'}
        ä¸»è¦è´¡çŒ®ï¼š{', '.join(paper_analysis.contributions) if paper_analysis.contributions else 'å¾…åˆ†æ'}

        è¦æ±‚ï¼š
        1. ç®€è¦æ€»ç»“è®ºæ–‡å†…å®¹
        2. æŒ‡å‡ºä¸»è¦ä¼˜ç‚¹å’Œä¸è¶³
        3. æå‡º2-3ä¸ªå…³é”®é—®é¢˜
        4. ç»™å‡ºæ€»ä½“è¯„ä»·
        """
        
        async for chunk in self._stream_llm(prompt):
            yield chunk

    async def _fallback_basic_review(self, pdf_content: str):
        """åŸºç¡€è¯„å®¡é™çº§ç­–ç•¥"""
        yield self._format_thought("âš ï¸ ä½¿ç”¨åŸºç¡€è¯„å®¡æ¨¡å¼...")
        
        # æå–æ–‡æœ¬
        text = self.document_processor.extract_text(pdf_content)
        if not text:
            yield self._format_content("æ— æ³•ä»PDFä¸­æå–æ–‡æœ¬å†…å®¹")
            return
        
        prompt = f"""
        è¯·å¯¹ä»¥ä¸‹è®ºæ–‡å†…å®¹è¿›è¡Œç®€è¦è¯„å®¡ï¼š

        è®ºæ–‡å†…å®¹ï¼š
        {text[:4000]}  # é™åˆ¶æ–‡æœ¬é•¿åº¦

        è¦æ±‚ï¼š
        1. æ€»ç»“è®ºæ–‡æ ¸å¿ƒå†…å®¹
        2. è¯„ä»·è®ºæ–‡è´¨é‡
        3. æå‡ºæ”¹è¿›å»ºè®®
        """
        
        async for chunk in self._stream_llm(prompt):
            yield chunk


class PaperAnalyzer:
    """è®ºæ–‡åˆ†æå™¨"""
    
    def __init__(self, document_processor: DocumentProcessor):
        self.document_processor = document_processor

    async def analyze(self, pdf_content: str) -> PaperAnalysis:
        """åˆ†æè®ºæ–‡å†…å®¹"""
        # æå–æ–‡æœ¬
        text = self.document_processor.extract_text(pdf_content)
        
        # åˆ†å—å¤„ç†
        chunks = self.document_processor.chunk_document(text)
        
        # è¯†åˆ«ç»“æ„
        structure = self.document_processor.identify_structure(chunks)
        
        # æå–å…³é”®å…ƒç´ 
        elements = self.document_processor.extract_key_elements(structure)
        
        return PaperAnalysis(
            structure=structure,
            contributions=elements.contributions,
            methodology=elements.methods,
            experiments=elements.datasets + elements.metrics,
            results=elements.findings,
            limitations=elements.limitations
        )


class RelatedWorkComparator:
    """ç›¸å…³å·¥ä½œæ¯”è¾ƒå™¨"""
    
    def __init__(self, academic_service: AcademicDataService, embedding_service: EmbeddingService):
        self.academic_service = academic_service
        self.embedding_service = embedding_service

    async def compare(self, paper_analysis: PaperAnalysis) -> List[Comparison]:
        """æ¯”è¾ƒç›¸å…³å·¥ä½œ"""
        if not paper_analysis.contributions:
            return []
        
        # åŸºäºè®ºæ–‡è´¡çŒ®æ£€ç´¢ç›¸å…³è®ºæ–‡
        query = " ".join(paper_analysis.contributions[:3])
        similar_papers = await self.academic_service.search_related_work(
            query, 
            "AI",  # é»˜è®¤é¢†åŸŸ
            limit=8
        )
        
        comparisons = []
        for paper in similar_papers[:5]:  # æ¯”è¾ƒå‰5ç¯‡ç›¸å…³è®ºæ–‡
            comparison = await self._compare_single_paper(paper_analysis, paper)
            comparisons.append(comparison)
        
        return sorted(comparisons, key=lambda x: x.similarity_score, reverse=True)

    async def _compare_single_paper(self, target: PaperAnalysis, other) -> Comparison:
        """æ¯”è¾ƒå•ç¯‡è®ºæ–‡"""
        aspects = {}
        
        # åˆ›æ–°æ€§æ¯”è¾ƒ
        aspects["novelty"] = await self._compare_novelty(target, other)
        
        # æ–¹æ³•æ¯”è¾ƒ
        aspects["methodology"] = await self._compare_methodology(target, other)
        
        # ç»“æœæ¯”è¾ƒ
        aspects["results"] = await self._compare_results(target, other)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity_score = await self._calculate_similarity(target, other)
        
        return Comparison(
            paper=other,
            aspects=aspects,
            similarity_score=similarity_score
        )

    async def _compare_novelty(self, target: PaperAnalysis, other) -> str:
        """æ¯”è¾ƒåˆ›æ–°æ€§"""
        # ç®€å•çš„åˆ›æ–°æ€§æ¯”è¾ƒ
        if target.contributions and other.abstract:
            target_text = " ".join(target.contributions)
            other_text = other.abstract or other.title or ""
            
            # ä½¿ç”¨åµŒå…¥è®¡ç®—ç›¸ä¼¼åº¦
            target_embedding = await self.embedding_service.get_embedding(target_text)
            other_embedding = await self.embedding_service.get_embedding(other_text)
            
            similarity = self.embedding_service.calculate_similarity(target_embedding, other_embedding)
            
            if similarity > 0.8:
                return "åˆ›æ–°æ€§è¾ƒä½ï¼Œä¸ç°æœ‰å·¥ä½œé«˜åº¦ç›¸ä¼¼"
            elif similarity > 0.5:
                return "æœ‰ä¸€å®šåˆ›æ–°æ€§ï¼Œä½†æ ¸å¿ƒæ€è·¯ç›¸è¿‘"
            else:
                return "åˆ›æ–°æ€§è¾ƒé«˜ï¼Œæå‡ºäº†æ–°çš„æ€è·¯"
        
        return "åˆ›æ–°æ€§å¾…è¯„ä¼°"

    async def _compare_methodology(self, target: PaperAnalysis, other) -> str:
        """æ¯”è¾ƒæ–¹æ³•"""
        if target.methodology and other.abstract:
            return "æ–¹æ³•å…·æœ‰ä¸€å®šç‹¬ç‰¹æ€§"
        return "æ–¹æ³•æ¯”è¾ƒå¾…æ·±å…¥åˆ†æ"

    async def _compare_results(self, target: PaperAnalysis, other) -> str:
        """æ¯”è¾ƒç»“æœ"""
        if target.results:
            return "å®éªŒç»“æœéœ€è¦æ›´å¤šå¯¹æ¯”åˆ†æ"
        return "ç»“æœå¯¹æ¯”ä¿¡æ¯ä¸è¶³"

    async def _calculate_similarity(self, target: PaperAnalysis, other) -> float:
        """è®¡ç®—ç›¸ä¼¼åº¦"""
        if target.contributions and other.abstract:
            target_text = " ".join(target.contributions)
            other_text = other.abstract or other.title or ""
            
            target_embedding = await self.embedding_service.get_embedding(target_text)
            other_embedding = await self.embedding_service.get_embedding(other_text)
            
            return self.embedding_service.calculate_similarity(target_embedding, other_embedding)
        
        return 0.0


class StructuredReviewGenerator:
    """ç»“æ„åŒ–è¯„å®¡ç”Ÿæˆå™¨"""
    
    def __init__(self, client):
        self.client = client

    async def generate(self, paper_analysis: PaperAnalysis, comparisons: List[Comparison], query: str) -> StructuredReview:
        """ç”Ÿæˆç»“æ„åŒ–è¯„å®¡"""
        # å¹¶è¡Œç”Ÿæˆå„è¯„å®¡éƒ¨åˆ†
        section_tasks = {
            "summary": self._generate_summary(paper_analysis, query),
            "strengths": self._generate_strengths(paper_analysis, comparisons),
            "weaknesses": self._generate_weaknesses(paper_analysis, comparisons),
            "questions": self._generate_questions(paper_analysis, comparisons)
        }
        
        section_results = {}
        for section, task in section_tasks.items():
            section_results[section] = await task
        
        # è®¡ç®—è¯„åˆ†
        scores = await self._calculate_scores(paper_analysis, comparisons)
        
        return StructuredReview(
            **section_results,
            scores=scores
        )

    async def _generate_summary(self, paper_analysis: PaperAnalysis, query: str) -> str:
        """ç”Ÿæˆæ‘˜è¦"""
        prompt = f"""
        åŸºäºä»¥ä¸‹è®ºæ–‡ä¿¡æ¯ï¼Œç”Ÿæˆè¯„å®¡æ‘˜è¦ï¼š

        è®ºæ–‡æ ‡é¢˜ï¼š{paper_analysis.structure.title or 'æœªè¯†åˆ«'}
        è®ºæ–‡æ‘˜è¦ï¼š{paper_analysis.structure.abstract or 'æœªæå–'}
        ä¸»è¦è´¡çŒ®ï¼š{', '.join(paper_analysis.contributions) if paper_analysis.contributions else 'å¾…åˆ†æ'}
        è¯„å®¡è¦æ±‚ï¼š{query}

        è¦æ±‚ï¼š
        1. ç®€è¦æ€»ç»“è®ºæ–‡æ ¸å¿ƒå†…å®¹
        2. çªå‡ºè®ºæ–‡çš„ä¸»è¦è´¡çŒ®
        3. ä¿æŒå®¢è§‚ä¸­ç«‹çš„è¯­æ°”
        4. æ§åˆ¶åœ¨200å­—ä»¥å†…
        """
        
        response = await self._call_llm(prompt)
        return response or "è®ºæ–‡æ‘˜è¦ç”Ÿæˆå¤±è´¥"

    async def _generate_strengths(self, paper_analysis: PaperAnalysis, comparisons: List[Comparison]) -> List[str]:
        """ç”Ÿæˆä¼˜ç‚¹åˆ—è¡¨"""
        prompt = f"""
        åŸºäºä»¥ä¸‹è®ºæ–‡ä¿¡æ¯ï¼ŒæŒ‡å‡ºè®ºæ–‡çš„ä¼˜ç‚¹ï¼š

        è®ºæ–‡è´¡çŒ®ï¼š{', '.join(paper_analysis.contributions) if paper_analysis.contributions else 'å¾…åˆ†æ'}
        ç ”ç©¶æ–¹æ³•ï¼š{', '.join(paper_analysis.methodology) if paper_analysis.methodology else 'å¾…åˆ†æ'}
        å®éªŒç»“æœï¼š{', '.join(paper_analysis.results) if paper_analysis.results else 'å¾…åˆ†æ'}

        ç›¸å…³æ¯”è¾ƒï¼š{len(comparisons)} ç¯‡ç›¸å…³è®ºæ–‡

        è¦æ±‚ï¼š
        1. åˆ—å‡º3-5ä¸ªä¸»è¦ä¼˜ç‚¹
        2. åŸºäºè®ºæ–‡å…·ä½“å†…å®¹
        3. è€ƒè™‘åˆ›æ–°æ€§ã€æŠ€æœ¯è´¨é‡ã€å®éªŒè®¾è®¡ç­‰æ–¹é¢
        4. æ¯ä¸ªä¼˜ç‚¹è¦å…·ä½“æ˜ç¡®
        """
        
        response = await self._call_llm(prompt)
        return self._parse_list_from_response(response)

    async def _generate_weaknesses(self, paper_analysis: PaperAnalysis, comparisons: List[Comparison]) -> List[str]:
        """ç”Ÿæˆç¼ºç‚¹åˆ—è¡¨"""
        prompt = f"""
        åŸºäºä»¥ä¸‹è®ºæ–‡ä¿¡æ¯ï¼ŒæŒ‡å‡ºè®ºæ–‡çš„ä¸è¶³å’Œæ”¹è¿›ç©ºé—´ï¼š

        è®ºæ–‡å±€é™æ€§ï¼š{', '.join(paper_analysis.limitations) if paper_analysis.limitations else 'å¾…åˆ†æ'}
        å®éªŒè®¾è®¡ï¼š{', '.join(paper_analysis.experiments) if paper_analysis.experiments else 'å¾…åˆ†æ'}
        ç›¸å…³æ¯”è¾ƒï¼š{len(comparisons)} ç¯‡ç›¸å…³è®ºæ–‡

        è¦æ±‚ï¼š
        1. åˆ—å‡º3-5ä¸ªä¸»è¦ä¸è¶³
        2. åŸºäºè®ºæ–‡å…·ä½“å†…å®¹
        3. è€ƒè™‘æ–¹æ³•å±€é™æ€§ã€å®éªŒä¸è¶³ã€åˆ†ææ·±åº¦ç­‰æ–¹é¢
        4. æ¯ä¸ªä¸è¶³è¦å…·ä½“æ˜ç¡®ï¼Œå¹¶æä¾›æ”¹è¿›å»ºè®®
        """
        
        response = await self._call_llm(prompt)
        return self._parse_list_from_response(response)

    async def _generate_questions(self, paper_analysis: PaperAnalysis, comparisons: List[Comparison]) -> List[str]:
        """ç”Ÿæˆé—®é¢˜åˆ—è¡¨"""
        prompt = f"""
        åŸºäºä»¥ä¸‹è®ºæ–‡ä¿¡æ¯ï¼Œæå‡ºéœ€è¦ä½œè€…æ¾„æ¸…çš„é—®é¢˜ï¼š

        è®ºæ–‡å†…å®¹ï¼š{paper_analysis.structure.abstract or 'æœªæå–'}
        æ–¹æ³•ç»†èŠ‚ï¼š{', '.join(paper_analysis.methodology) if paper_analysis.methodology else 'å¾…åˆ†æ'}
        å®éªŒç»“æœï¼š{', '.join(paper_analysis.results) if paper_analysis.results else 'å¾…åˆ†æ'}

        è¦æ±‚ï¼š
        1. æå‡º3-5ä¸ªå…³é”®é—®é¢˜
        2. é—®é¢˜è¦å…·ä½“ä¸”æœ‰æ·±åº¦
        3. å…³æ³¨æ–¹æ³•ç»†èŠ‚ã€å®éªŒè®¾ç½®ã€ç»“æœè§£é‡Šç­‰æ–¹é¢
        4. å¸®åŠ©æ”¹è¿›è®ºæ–‡è´¨é‡
        """
        
        response = await self._call_llm(prompt)
        return self._parse_list_from_response(response)

    async def _calculate_scores(self, paper_analysis: PaperAnalysis, comparisons: List[Comparison]) -> ReviewScores:
        """è®¡ç®—è¯„åˆ†"""
        # åŸºäºè®ºæ–‡è´¨é‡å’Œæ¯”è¾ƒç»“æœçš„ç®€å•è¯„åˆ†
        novelty = await self._score_novelty(paper_analysis, comparisons)
        technical_quality = await self._score_technical_quality(paper_analysis)
        clarity = await self._score_clarity(paper_analysis)
        
        # è®¡ç®—æ€»ä½“è¯„åˆ†
        overall = (novelty + technical_quality + clarity) / 3
        confidence = 4.0  # é»˜è®¤ç½®ä¿¡åº¦
        
        return ReviewScores(
            overall=overall,
            novelty=novelty,
            technical_quality=technical_quality,
            clarity=clarity,
            confidence=confidence
        )

    async def _score_novelty(self, paper_analysis: PaperAnalysis, comparisons: List[Comparison]) -> float:
        """è¯„åˆ†åˆ›æ–°æ€§"""
        if not comparisons:
            return 7.0
        
        # åŸºäºç›¸ä¼¼åº¦è¯„åˆ†åˆ›æ–°æ€§
        avg_similarity = sum(comp.similarity_score for comp in comparisons) / len(comparisons)
        
        if avg_similarity > 0.8:
            return 5.0  # åˆ›æ–°æ€§è¾ƒä½
        elif avg_similarity > 0.6:
            return 6.5  # æœ‰ä¸€å®šåˆ›æ–°æ€§
        elif avg_similarity > 0.4:
            return 7.5  # åˆ›æ–°æ€§è¾ƒå¥½
        else:
            return 8.5  # åˆ›æ–°æ€§å¾ˆé«˜

    async def _score_technical_quality(self, paper_analysis: PaperAnalysis) -> float:
        """è¯„åˆ†æŠ€æœ¯è´¨é‡"""
        score = 7.0  # åŸºç¡€åˆ†
        
        # åŸºäºæ–¹æ³•å®Œæ•´æ€§åŠ åˆ†
        if paper_analysis.methodology and len(paper_analysis.methodology) >= 2:
            score += 0.5
        
        # åŸºäºå®éªŒè®¾è®¡åŠ åˆ†
        if paper_analysis.experiments and len(paper_analysis.experiments) >= 2:
            score += 0.5
        
        # åŸºäºç»“æœåˆ†æåŠ åˆ†
        if paper_analysis.results and len(paper_analysis.results) >= 2:
            score += 0.5
        
        return min(score, 9.5)

    async def _score_clarity(self, paper_analysis: PaperAnalysis) -> float:
        """è¯„åˆ†æ¸…æ™°åº¦"""
        score = 7.0  # åŸºç¡€åˆ†
        
        # åŸºäºç»“æ„å®Œæ•´æ€§åŠ åˆ†
        if (paper_analysis.structure.abstract and 
            paper_analysis.structure.methodology and 
            paper_analysis.structure.results):
            score += 1.0
        
        # åŸºäºè´¡çŒ®æ˜ç¡®æ€§åŠ åˆ†
        if paper_analysis.contributions and len(paper_analysis.contributions) >= 2:
            score += 0.5
        
        return min(score, 9.0)

    async def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨LLM"""
        try:
            response = await self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1000,
                temperature=0.7
            )
            return response.choices[0].message.content
        except Exception as e:
            logging.error(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
            return ""

    def _parse_list_from_response(self, response: str) -> List[str]:
        """ä»å“åº”ä¸­è§£æåˆ—è¡¨"""
        items = []
        lines = response.split('\n')
        
        for line in lines:
            line = line.strip()
            # åŒ¹é…åˆ—è¡¨é¡¹æ ¼å¼
            if line.startswith('- ') or line.startswith('â€¢ ') or line.startswith('* '):
                items.append(line[2:].strip())
            elif line and len(line) > 10:  # éç©ºä¸”æœ‰ä¸€å®šé•¿åº¦
                items.append(line)
        
        return items[:5]  # é™åˆ¶æ•°é‡


class ReviewValidator:
    """è¯„å®¡éªŒè¯å™¨"""
    
    async def validate(self, review: StructuredReview) -> bool:
        """éªŒè¯è¯„å®¡å®Œæ•´æ€§"""
        if not review.summary or len(review.summary.strip()) < 50:
            return False
        
        if not review.strengths or len(review.strengths) < 2:
            return False
        
        if not review.weaknesses or len(review.weaknesses) < 2:
            return False
        
        if not review.questions or len(review.questions) < 2:
            return False
        
        # éªŒè¯è¯„åˆ†èŒƒå›´
        scores = review.scores
        if not (0 <= scores.overall <= 10 and
                0 <= scores.novelty <= 10 and
                0 <= scores.technical_quality <= 10 and
                0 <= scores.clarity <= 10 and
                0 <= scores.confidence <= 5):
            return False
        
        return True
