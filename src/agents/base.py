import asyncio
import json
import logging
from abc import ABC, abstractmethod
from typing import AsyncGenerator, List, Dict, Any, Optional
from datetime import datetime

from openai import AsyncOpenAI

from ..config import config, TimeoutConfig
from ..models import (
    ThoughtStep, AgentConfig, ErrorResponse, StreamChunk
)


class AgentBase(ABC):
    """æ™ºèƒ½ä½“åŸºç±» - æä¾›ç»Ÿä¸€çš„æ™ºèƒ½ä½“æ¥å£å’Œæ ¸å¿ƒåŠŸèƒ½"""
    
    def __init__(self, agent_config: AgentConfig):
        self.config = agent_config
        self.logger = logging.getLogger(f"agent.{agent_config.name}")
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ - ä½¿ç”¨å…¨å±€configå¯¹è±¡
        self.client = AsyncOpenAI(
            base_url=config.SCI_MODEL_BASE_URL,
            api_key=config.SCI_MODEL_API_KEY
        )
        
        # åˆå§‹åŒ–åµŒå…¥å®¢æˆ·ç«¯ - ä½¿ç”¨å…¨å±€configå¯¹è±¡
        self.embedding_client = AsyncOpenAI(
            base_url=config.SCI_EMBEDDING_BASE_URL,
            api_key=config.SCI_EMBEDDING_API_KEY
        )
        
        # æ€è€ƒæ­¥éª¤è®°å½•
        self.thought_steps: List[ThoughtStep] = []
        
        # ä¼šè¯çŠ¶æ€
        self.session_start_time = datetime.now()
        self.is_streaming = agent_config.enable_streaming

    @abstractmethod
    async def execute(self, query: str, **kwargs) -> AsyncGenerator[str, None]:
        """æ‰§è¡Œæ™ºèƒ½ä½“ä»»åŠ¡ - æŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°"""
        pass

    async def _thinking_loop(self) -> AsyncGenerator[str, None]:
        """æ€è€ƒå¾ªç¯ - å­ç±»å¯ä»¥é‡å†™æ­¤æ–¹æ³•å®ç°è‡ªå®šä¹‰æ€è€ƒé€»è¾‘"""
        steps = []
        
        # åˆ†æé˜¶æ®µ
        analysis_step = ThoughtStep(
            step_type="analysis",
            content="åˆ†æç”¨æˆ·æŸ¥è¯¢å’Œä»»åŠ¡éœ€æ±‚...",
            timestamp=datetime.now()
        )
        steps.append(analysis_step)
        yield self._format_thought(analysis_step.content)
        
        # æ£€ç´¢é˜¶æ®µ
        retrieval_step = ThoughtStep(
            step_type="retrieval",
            content="æ£€ç´¢ç›¸å…³çŸ¥è¯†å’Œä¿¡æ¯...",
            timestamp=datetime.now()
        )
        steps.append(retrieval_step)
        yield self._format_thought(retrieval_step.content)
        
        # ç”Ÿæˆé˜¶æ®µ
        generation_step = ThoughtStep(
            step_type="generation",
            content="ç”Ÿæˆè§£å†³æ–¹æ¡ˆ...",
            timestamp=datetime.now()
        )
        steps.append(generation_step)
        yield self._format_thought(generation_step.content)
        
        self.thought_steps = steps

    async def _fallback_strategy(self, query: str, **kwargs) -> AsyncGenerator[str, None]:
        """é™çº§ç­–ç•¥ - å½“ä¸»è¦æ–¹æ³•å¤±è´¥æ—¶ä½¿ç”¨"""
        self.logger.warning(f"ä½¿ç”¨é™çº§ç­–ç•¥å¤„ç†æŸ¥è¯¢: {query}")
        
        yield self._format_thought("âš ï¸ ç³»ç»Ÿé‡åˆ°é—®é¢˜ï¼Œä½¿ç”¨ç®€åŒ–æ¨¡å¼å¤„ç†...")
        
        # ç®€åŒ–æç¤ºè¯
        simple_prompt = f"""
        è¯·åŸºäºä»¥ä¸‹æŸ¥è¯¢æä¾›ç®€è¦å›ç­”ï¼š
        
        æŸ¥è¯¢ï¼š{query}
        
        è¦æ±‚ï¼š
        1. æä¾›ç®€æ´æ˜äº†çš„å›ç­”
        2. ä¸“æ³¨äºæ ¸å¿ƒè¦ç‚¹
        3. é¿å…å¤æ‚åˆ†æ
        """
        
        try:
            stream = await self.client.chat.completions.create(
                model=self.config.model,
                messages=[{"role": "user", "content": simple_prompt}],
                max_tokens=1000,
                temperature=0.7,
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta_content = chunk.choices[0].delta.content
                    if delta_content:
                        yield self._format_content(delta_content)
                        
        except Exception as e:
            error_msg = f"é™çº§ç­–ç•¥ä¹Ÿå¤±è´¥äº†: {str(e)}"
            self.logger.error(error_msg)
            yield self._format_thought(f"âŒ {error_msg}")

    def _format_sse_data(self, content: str) -> str:
        """æ ¼å¼åŒ–SSEæ•°æ®"""
        response_data = StreamChunk(
            choices=[{"delta": {"content": content}}]
        )
        return f"data: {response_data.json()}\n\n"

    def _format_thought(self, content: str) -> str:
        """æ ¼å¼åŒ–æ€è€ƒå†…å®¹"""
        thought_data = {
            "object": "chat.completion.chunk",
            "choices": [{
                "delta": {
                    "role": "assistant",
                    "content": f"\n\nğŸ¤” {content}\n\n"
                }
            }]
        }
        return f"data: {json.dumps(thought_data)}\n\n"

    def _format_content(self, content: str) -> str:
        """æ ¼å¼åŒ–æ™®é€šå†…å®¹"""
        return self._format_sse_data(content)

    async def _handle_timeout(self) -> AsyncGenerator[str, None]:
        """è¶…æ—¶å¤„ç†"""
        self.logger.warning("ä»»åŠ¡æ‰§è¡Œè¶…æ—¶")
        yield self._format_thought("â° æ—¶é—´é™åˆ¶å·²åˆ°ï¼Œè¾“å‡ºå½“å‰æœ€ä½³ç»“æœ...")
        
        # è¾“å‡ºå½“å‰æ€è€ƒæ­¥éª¤çš„æ€»ç»“
        if self.thought_steps:
            summary = "åŸºäºå½“å‰åˆ†æï¼Œä¸»è¦å‘ç°åŒ…æ‹¬ï¼š\n"
            for step in self.thought_steps[-3:]:  # å–æœ€å3ä¸ªæ­¥éª¤
                summary += f"- {step.content}\n"
            yield self._format_content(summary)

    def _validate_input(self, query: str, **kwargs) -> bool:
        """è¾“å…¥éªŒè¯"""
        if not query or not query.strip():
            self.logger.error("æŸ¥è¯¢å†…å®¹ä¸ºç©º")
            return False
            
        if len(query.strip()) < 3:
            self.logger.error("æŸ¥è¯¢å†…å®¹è¿‡çŸ­")
            return False
            
        return True

    def _log_operation(self, operation: str, duration: float, success: bool):
        """è®°å½•æ“ä½œæ—¥å¿—"""
        self.logger.info(
            f"Agent operation completed",
            extra={
                "agent": self.config.name,
                "operation": operation,
                "duration": duration,
                "success": success,
                "timestamp": datetime.now().isoformat()
            }
        )

    async def _get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬åµŒå…¥å‘é‡"""
        try:
            response = await self.embedding_client.embeddings.create(
                model=config.SCI_EMBEDDING_MODEL,
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            self.logger.error(f"è·å–åµŒå…¥å‘é‡å¤±è´¥: {str(e)}")
            return []

    async def _call_llm(self, prompt: str, model: Optional[str] = None, **kwargs) -> str:
        """è°ƒç”¨LLMæ¨¡å‹"""
        try:
            model_to_use = model or self.config.model
            temperature = kwargs.get('temperature', self.config.temperature)
            max_tokens = kwargs.get('max_tokens', self.config.max_tokens)
            
            response = await self.client.chat.completions.create(
                model=model_to_use,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=temperature,
                stream=False
            )
            
            return response.choices[0].message.content
        except Exception as e:
            self.logger.error(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    async def _stream_llm(self, prompt: str, model: Optional[str] = None, **kwargs) -> AsyncGenerator[str, None]:
        """æµå¼è°ƒç”¨LLMæ¨¡å‹"""
        try:
            model_to_use = model or self.config.model
            temperature = kwargs.get('temperature', self.config.temperature)
            max_tokens = kwargs.get('max_tokens', self.config.max_tokens)
            
            stream = await self.client.chat.completions.create(
                model=model_to_use,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=temperature,
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta_content = chunk.choices[0].delta.content
                    if delta_content:
                        yield self._format_content(delta_content)
                        
        except Exception as e:
            self.logger.error(f"æµå¼LLMè°ƒç”¨å¤±è´¥: {str(e)}")
            yield self._format_thought(f"âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}")

    def _calculate_session_duration(self) -> float:
        """è®¡ç®—ä¼šè¯æŒç»­æ—¶é—´"""
        return (datetime.now() - self.session_start_time).total_seconds()
